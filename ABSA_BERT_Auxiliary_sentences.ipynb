{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABSA-BERT-Auxiliary-sentences.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnxVpAuBnc3Q",
        "colab_type": "code",
        "outputId": "861b2e16-a2a7-44df-bbc9-28df3e586f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpTIr7CpoZrP",
        "colab_type": "code",
        "outputId": "94c02379-dfda-475c-e46b-87135ef256ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.39)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.15.39)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.39->boto3->pytorch-pretrained-bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6s8ocvPdtXv",
        "colab_type": "code",
        "outputId": "8a66795d-31bd-4fd7-d16b-f84b358794dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.39)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.39)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFMkICw7ocT9",
        "colab_type": "code",
        "outputId": "0ba9bb8a-0ae5-4507-c831-6ac4a3451e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig, BertModel, BertForSequenceClassification, AdamW\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5iwC4LDoevm",
        "colab_type": "code",
        "outputId": "e651adb5-f96f-4fb2-fab8-cd35c3316660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcd2Fif3pqcw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e5572e8b-55fe-4fa3-a0f4-f0d6c9676aa2"
      },
      "source": [
        "df = pd.read_csv('data/traindata.csv', sep='\\t', header=0, names=['polarity', 'aspect', 'target', 'position', 'sentence'])\n",
        "df.head(3)\n",
        "\n",
        "df_dev = pd.read_csv('data/devdata.csv', sep='\\t', header=0, names=['polarity', 'aspect', 'target', 'position', 'sentence'])\n",
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>aspect</th>\n",
              "      <th>target</th>\n",
              "      <th>position</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>AMBIENCE#GENERAL</td>\n",
              "      <td>trattoria</td>\n",
              "      <td>25:34</td>\n",
              "      <td>This quaint and romantic trattoria is at the t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>food</td>\n",
              "      <td>98:102</td>\n",
              "      <td>The have over 100 different beers to offer thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>STAFF</td>\n",
              "      <td>5:10</td>\n",
              "      <td>THIS STAFF SHOULD BE FIRED.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#STYLE_OPTIONS</td>\n",
              "      <td>menu</td>\n",
              "      <td>4:8</td>\n",
              "      <td>The menu looked great, and the waiter was very...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>tuna</td>\n",
              "      <td>4:8</td>\n",
              "      <td>The tuna and wasabe potatoes are excellent.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>positive</td>\n",
              "      <td>DRINKS#QUALITY</td>\n",
              "      <td>expresso</td>\n",
              "      <td>29:37</td>\n",
              "      <td>One of us actually liked the expresso - that's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>negative</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>waitress</td>\n",
              "      <td>20:28</td>\n",
              "      <td>The hostess and the waitress were incredibly r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>positive</td>\n",
              "      <td>RESTAURANT#PRICES</td>\n",
              "      <td>place</td>\n",
              "      <td>12:17</td>\n",
              "      <td>this little place has a cute interior decor an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1500</th>\n",
              "      <td>positive</td>\n",
              "      <td>RESTAURANT#GENERAL</td>\n",
              "      <td>restaurant</td>\n",
              "      <td>30:40</td>\n",
              "      <td>Nice Family owned traditional restaurant.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1501</th>\n",
              "      <td>positive</td>\n",
              "      <td>AMBIENCE#GENERAL</td>\n",
              "      <td>atmosphere</td>\n",
              "      <td>74:84</td>\n",
              "      <td>The first time I went, and was completely take...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1502 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      polarity  ...                                           sentence\n",
              "0     positive  ...  This quaint and romantic trattoria is at the t...\n",
              "1     positive  ...  The have over 100 different beers to offer thi...\n",
              "2     negative  ...                        THIS STAFF SHOULD BE FIRED.\n",
              "3     positive  ...  The menu looked great, and the waiter was very...\n",
              "4     positive  ...        The tuna and wasabe potatoes are excellent.\n",
              "...        ...  ...                                                ...\n",
              "1497  positive  ...  One of us actually liked the expresso - that's...\n",
              "1498  negative  ...  The hostess and the waitress were incredibly r...\n",
              "1499  positive  ...  this little place has a cute interior decor an...\n",
              "1500  positive  ...          Nice Family owned traditional restaurant.\n",
              "1501  positive  ...  The first time I went, and was completely take...\n",
              "\n",
              "[1502 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ziQf-XhFFN5r",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-large-uncased', \n",
        "    do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZS1G6rn-wbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "711e164e-b3ec-4c33-d777-218b1dd7fe14"
      },
      "source": [
        "tokenizer.add_tokens(list(df.aspect.str.lower().unique()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu_dlKP638QX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "7d74f936-ce71-48a9-e875-120cd3b893ec"
      },
      "source": [
        "list(df.aspect.str.lower().unique())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ambience#general',\n",
              " 'food#quality',\n",
              " 'service#general',\n",
              " 'food#style_options',\n",
              " 'drinks#quality',\n",
              " 'restaurant#miscellaneous',\n",
              " 'restaurant#general',\n",
              " 'drinks#prices',\n",
              " 'food#prices',\n",
              " 'location#general',\n",
              " 'drinks#style_options',\n",
              " 'restaurant#prices']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbTjlbx-EuW-",
        "colab_type": "code",
        "outputId": "4dfcbc2f-61ff-4c90-a8e1-bd2d67bbd3b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "def create_first_input_ids(df):\n",
        "  first_sentences = df.sentence.values\n",
        "  first_tokens = [tokenizer.tokenize(sentence) for sentence in first_sentences]\n",
        "  first_input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in first_tokens]\n",
        "  print('first_sentences: ', first_sentences[3], '\\nfirst_input_ids: ', first_input_ids[3])\n",
        "  return first_input_ids\n",
        "\n",
        "first_input_ids = create_first_input_ids(df)\n",
        "first_input_ids_dev = create_first_input_ids(df_dev)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first_sentences:  The menu looked great, and the waiter was very nice, but when the food came, it was average. \n",
            "first_input_ids:  [1996, 12183, 2246, 2307, 1010, 1998, 1996, 15610, 2001, 2200, 3835, 1010, 2021, 2043, 1996, 2833, 2234, 1010, 2009, 2001, 2779, 1012]\n",
            "first_sentences:  The food we ordered was excellent, although I wouldn't say the margaritas were anything to write home about. \n",
            "first_input_ids:  [1996, 2833, 2057, 3641, 2001, 6581, 1010, 2348, 1045, 2876, 1005, 1056, 2360, 1996, 24570, 2015, 2020, 2505, 2000, 4339, 2188, 2055, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efUluKklwmoK",
        "colab_type": "code",
        "outputId": "d033ac69-2fe4-408b-ddf4-07ea246bdd06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "def create_auxiliary_input_ids(df):\n",
        "  auxiliary_sentences = [\"\" + aspect + \" - \" + target \n",
        "                         for aspect,target \n",
        "                         in list(zip(df.aspect.str.lower().values, df.target.str.lower().values))]\n",
        "  # auxiliary_sentences = ['neutral' for aspect,target \n",
        "  #                        in list(zip(df.aspect.values, df.target.values))]\n",
        "  auxiliary_tokens = [tokenizer.tokenize(sentence) for sentence in auxiliary_sentences]\n",
        "  auxiliary_input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in auxiliary_tokens]\n",
        "  print('auxiliary_sentence: ', auxiliary_sentences[0], '\\nauxiliary_tokens: ', auxiliary_tokens[0], '\\nauxiliary_input_ids: ', auxiliary_input_ids[0])  \n",
        "  return auxiliary_input_ids\n",
        "\n",
        "auxiliary_input_ids = create_auxiliary_input_ids(df)\n",
        "auxiliary_input_ids_dev = create_auxiliary_input_ids(df_dev)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "auxiliary_sentence:  ambience#general - trattoria \n",
            "auxiliary_tokens:  ['ambience#general', '-', 'tr', '##att', '##oria'] \n",
            "auxiliary_input_ids:  [30522, 1011, 19817, 19321, 11069]\n",
            "auxiliary_sentence:  restaurant#general - place \n",
            "auxiliary_tokens:  ['restaurant#general', '-', 'place'] \n",
            "auxiliary_input_ids:  [30528, 1011, 2173]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buAZJuLXmHSc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7200901e-efe7-41d4-c1ea-cc4fad3d534a"
      },
      "source": [
        "auxiliary_input_ids"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[30522, 1011, 19817, 19321, 11069],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 3095],\n",
              " [30525, 1011, 12183],\n",
              " [30523, 1011, 24799],\n",
              " [30524, 1011, 3095],\n",
              " [30524, 1011, 2326],\n",
              " [30525, 1011, 22861, 4160, 10335],\n",
              " [30522, 1011, 2173],\n",
              " [30523, 1011, 10439, 20624, 6290, 1997, 9724, 2015],\n",
              " [30523, 1011, 9440],\n",
              " [30526, 1011, 8974],\n",
              " [30523, 1011, 9372],\n",
              " [30523, 1011, 5431, 8081, 2063, 12183],\n",
              " [30523, 1011, 8040, 8095, 11923],\n",
              " [30522, 1011, 2572, 11283, 5897],\n",
              " [30522, 1011, 2397, 2305, 7224],\n",
              " [30523, 1011, 28253, 17153, 2618],\n",
              " [30523, 1011, 25482, 3059, 8808],\n",
              " [30523, 1011, 5371, 2102],\n",
              " [30523, 1011, 10861, 3736, 4305, 4571],\n",
              " [30527, 1011, 2173],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 4524, 9050],\n",
              " [30528, 1011, 4825],\n",
              " [30523, 1011, 2413, 22201],\n",
              " [30523, 1011, 5785, 10447],\n",
              " [30522, 1011, 2572, 11283, 5897],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 15610],\n",
              " [30524, 1011, 17917, 2278],\n",
              " [30523, 1011, 23621],\n",
              " [30526, 1011, 4392],\n",
              " [30523, 1011, 9372],\n",
              " [30528, 1011, 2173],\n",
              " [30524, 1011, 3524],\n",
              " [30528, 1011, 2796, 4825],\n",
              " [30523, 1011, 28144, 8820, 2015],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30527, 1011, 28879, 1005, 1055],\n",
              " [30523, 1011, 2712, 22083, 2015],\n",
              " [30526, 1011, 4511, 2011, 1996, 3221],\n",
              " [30528, 1011, 4825],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 7273],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 8013, 2326],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 17488, 20934, 22510, 2050],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 10733],\n",
              " [30522, 1011, 17768, 2050, 1005, 1055],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 27940],\n",
              " [30524, 1011, 2326],\n",
              " [30522, 1011, 4592, 25545],\n",
              " [30523, 1011, 3424, 1011, 24857],\n",
              " [30523, 1011, 10733],\n",
              " [30522, 1011, 2364, 7759, 2282],\n",
              " [30522, 1011, 25545],\n",
              " [30522, 1011, 2189],\n",
              " [30524, 1011, 2326],\n",
              " [30525, 1011, 19247, 12183, 2015],\n",
              " [30525, 1011, 7987, 2271, 20318, 10230],\n",
              " [30529, 1011, 2160, 12327],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30530, 1011, 2833],\n",
              " [30525, 1011, 2833],\n",
              " [30522, 1011, 7224],\n",
              " [30527, 1011, 2173],\n",
              " [30527, 1011, 2173],\n",
              " [30524, 1011, 15610, 2015],\n",
              " [30523, 1011, 2980, 6077],\n",
              " [30526, 1011, 8974],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 3968, 8067, 4168, 5760, 2098],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 10447],\n",
              " [30524, 1011, 2149, 3270],\n",
              " [30523, 1011, 10447],\n",
              " [30524, 1011, 13877],\n",
              " [30522, 1011, 3065],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 13877],\n",
              " [30525, 1011, 10733],\n",
              " [30525, 1011, 2887, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30531, 1011, 3193],\n",
              " [30526, 1011, 8974],\n",
              " [30523, 1011, 12183],\n",
              " [30524, 1011, 15610],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 15960, 2061, 2226, 2615, 23451, 2072],\n",
              " [30528, 1011, 4840, 4825],\n",
              " [30522, 1011, 4825],\n",
              " [30523, 1011, 1042, 10448, 2063, 24665, 3022, 26568, 2638, 2007, 20965, 2015],\n",
              " [30527, 1011, 5069],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 2514],\n",
              " [30524, 1011, 3208],\n",
              " [30524, 1011, 22566],\n",
              " [30523, 1011, 20919, 3869],\n",
              " [30530, 1011, 25482, 24799, 4897],\n",
              " [30524, 1011, 2111],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 2173],\n",
              " [30528, 1011, 6506],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 2833],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 11604, 16521],\n",
              " [30522, 1011, 2648, 2795],\n",
              " [30522, 1011, 7579, 5613, 2265],\n",
              " [30522, 1011, 2173],\n",
              " [30523, 1011, 14208, 25957, 3560, 2232],\n",
              " [30523, 1011, 10733],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30523, 1011, 4524, 9050],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 2715, 2887],\n",
              " [30530, 1011, 2035, 2017, 2064, 4521, 10514, 6182],\n",
              " [30525, 1011, 7975, 28774, 8717, 3211],\n",
              " [30529, 1011, 4511, 2862],\n",
              " [30523, 1011, 2833],\n",
              " [30532, 1011, 4511, 2862],\n",
              " [30522, 1011, 2173],\n",
              " [30524, 1011, 21597],\n",
              " [30528, 1011, 14124, 2474, 26893],\n",
              " [30526, 1011, 8739, 1521, 1055],\n",
              " [30523, 1011, 10144, 4897],\n",
              " [30530, 1011, 11737, 7680],\n",
              " [30522, 1011, 27218, 17369, 7869],\n",
              " [30528, 1011, 5069],\n",
              " [30523, 1011, 7954],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 10733],\n",
              " [30524, 1011, 6959],\n",
              " [30523, 1011, 26478, 4402],\n",
              " [30524, 1011, 3095],\n",
              " [30533, 1011, 2173],\n",
              " [30524, 1011, 3524],\n",
              " [30530, 1011, 26478, 4402],\n",
              " [30527, 1011, 3095],\n",
              " [30524, 1011, 2326],\n",
              " [30527, 1011, 3295],\n",
              " [30522, 1011, 2572, 11283, 5897],\n",
              " [30523, 1011, 2413, 20377, 13181, 13258],\n",
              " [30532, 1011, 4511, 2862],\n",
              " [30533, 1011, 4825],\n",
              " [30524, 1011, 2326],\n",
              " [30525, 1011, 9372],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30522, 1011, 25545],\n",
              " [30522, 1011, 7224],\n",
              " [30523, 1011, 10733],\n",
              " [30524, 1011, 13877],\n",
              " [30523, 1011, 2833],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 26666, 2007, 8040, 8095, 11923, 1998, 20130],\n",
              " [30522, 1011, 2173],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 2173],\n",
              " [30524, 1011, 13877],\n",
              " [30524, 1011, 21584],\n",
              " [30523, 1011, 7975, 24857],\n",
              " [30522, 1011, 2686],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 3208],\n",
              " [30523, 1011, 4989, 1997, 4857, 19116, 10733],\n",
              " [30522, 1011, 2717, 4648, 3372],\n",
              " [30523, 1011, 2204],\n",
              " [30523, 1011, 2796, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 15610],\n",
              " [30522, 1011, 13541, 1011, 2422],\n",
              " [30524, 1011, 2326],\n",
              " [30525, 1011, 10447],\n",
              " [30530, 1011, 2035, 2017, 2064, 4521, 3066],\n",
              " [30528, 1011, 2173],\n",
              " [30528, 1011, 2173],\n",
              " [30532, 1011, 4511, 4989],\n",
              " [30533, 1011, 1996, 2176, 3692],\n",
              " [30528, 1011, 18543, 4825],\n",
              " [30524, 1011, 13877],\n",
              " [30523, 1011, 20861],\n",
              " [30523, 1011, 2665, 15478, 2007, 11546],\n",
              " [30524, 1011, 18074, 22542],\n",
              " [30526, 1011, 24480, 2015],\n",
              " [30523, 1011, 20482, 11840, 1998, 20944, 10439, 20624, 6290],\n",
              " [30522, 1011, 16729, 29493, 4048],\n",
              " [30525, 1011, 24857, 2015],\n",
              " [30523, 1011, 18064],\n",
              " [30523, 1011, 11840],\n",
              " [30524, 1011, 6959],\n",
              " [30523, 1011, 4596],\n",
              " [30532, 1011, 3347],\n",
              " [30523, 1011, 16521, 2015],\n",
              " [30528, 1011, 2173],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 20739, 2080, 10733],\n",
              " [30522, 1011, 2572, 15599, 3401],\n",
              " [30525, 1011, 24857, 9502, 2063],\n",
              " [30528, 1011, 5341, 4894],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 7708, 2304, 23605, 9587, 17854, 2063],\n",
              " [30522, 1011, 2173],\n",
              " [30523, 1011, 12846],\n",
              " [30525, 1011, 4664, 10826],\n",
              " [30522, 1011, 25545],\n",
              " [30527, 1011, 2686],\n",
              " [30530, 1011, 7954],\n",
              " [30523, 1011, 14380, 16521],\n",
              " [30525, 1011, 4524, 9050],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30527, 1011, 14124, 2474, 26893],\n",
              " [30522, 1011, 13787, 25444],\n",
              " [30523, 1011, 2796, 2822, 2833],\n",
              " [30523, 1011, 3756, 14162],\n",
              " [30523, 1011, 2431, 3976, 10514, 6182, 3066],\n",
              " [30523, 1011, 7954],\n",
              " [30523, 1011, 2833],\n",
              " [30527, 1011, 2173],\n",
              " [30523, 1011, 6949, 8808, 2015],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 27940],\n",
              " [30522, 1011, 4044],\n",
              " [30528, 1011, 2300, 1005, 1055, 3341],\n",
              " [30523, 1011, 26192, 2094, 2304, 19429],\n",
              " [30523, 1011, 4524, 9050],\n",
              " [30528, 1011, 21146, 8569, 1011, 21146, 8569, 4825],\n",
              " [30523, 1011, 11687, 7273],\n",
              " [30522, 1011, 2572, 11283, 5897],\n",
              " [30530, 1011, 2833],\n",
              " [30522, 1011, 2173],\n",
              " [30523, 1011, 5869, 8490, 2532, 12183],\n",
              " [30528, 1011, 2173],\n",
              " [30522, 1011, 4292],\n",
              " [30525, 1011, 2035, 2017, 2064, 4521, 3066],\n",
              " [30523, 1011, 10250, 8067, 3089],\n",
              " [30523, 1011, 8808, 4645, 2121],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 3524, 3095],\n",
              " [30524, 1011, 3095],\n",
              " [30524, 1011, 2326],\n",
              " [30532, 1011, 4511, 2862],\n",
              " [30525, 1011, 4524, 2884],\n",
              " [30532, 1011, 4511, 2862],\n",
              " [30524, 1011, 2326],\n",
              " [30528, 1011, 16950, 9468, 5886, 2080, 13433, 5302, 7983, 2072],\n",
              " [30523, 1011, 20130, 8040, 16613, 2072],\n",
              " [30524, 1011, 3095],\n",
              " [30525, 1011, 2833],\n",
              " [30528, 1011, 7668, 17688],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 2189],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2392, 1997, 2160, 3095],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 2326],\n",
              " [30530, 1011, 4596],\n",
              " [30524, 1011, 15610],\n",
              " [30525, 1011, 22286, 2015],\n",
              " [30523, 1011, 7273, 2833],\n",
              " [30524, 1011, 10026],\n",
              " [30523, 1011, 10733],\n",
              " [30531, 1011, 3295],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 14704, 1997, 16392, 10733],\n",
              " [30523, 1011, 4895, 2072, 2192, 4897],\n",
              " [30528, 1011, 14124, 2474, 26893],\n",
              " [30533, 1011, 8782, 17378],\n",
              " [30523, 1011, 10447],\n",
              " [30524, 1011, 3095],\n",
              " [30523, 1011, 10250, 8067, 3089],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 23621, 21719],\n",
              " [30523, 1011, 10250, 15975, 2015],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 3524],\n",
              " [30528, 1011, 4825],\n",
              " [30524, 1011, 2111],\n",
              " [30524, 1011, 13877, 2229],\n",
              " [30522, 1011, 21209],\n",
              " [30528, 1011, 4774, 7273],\n",
              " [30525, 1011, 3968, 8067, 4168, 5760, 2098],\n",
              " [30522, 1011, 25545],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 4100, 1011, 2797, 2119, 2015],\n",
              " [30525, 1011, 8810],\n",
              " [30522, 1011, 25545],\n",
              " [30525, 1011, 12241, 6562, 2015],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2111],\n",
              " [30524, 1011, 14736, 7913, 1040, 1005],\n",
              " [30522, 1011, 25545],\n",
              " [30525, 1011, 8808, 4645, 2121],\n",
              " [30524, 1011, 3095],\n",
              " [30522, 1011, 2173],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 7975, 19354, 9305, 9541],\n",
              " [30525, 1011, 26192, 2094, 5003, 4048, 5003, 4048],\n",
              " [30523, 1011, 4596],\n",
              " [30533, 1011, 4825],\n",
              " [30524, 1011, 3095],\n",
              " [30523, 1011, 10514, 6182, 4328, 12731, 24894, 5677, 4897],\n",
              " [30522, 1011, 7224],\n",
              " [30530, 1011, 2833],\n",
              " [30533, 1011, 2173],\n",
              " [30524, 1011, 2326],\n",
              " [30533, 1011, 4825],\n",
              " [30526, 1011, 8974],\n",
              " [30525, 1011, 9372],\n",
              " [30523, 1011, 9372],\n",
              " [30533, 1011, 14124, 2474, 26893],\n",
              " [30524, 1011, 15610],\n",
              " [30525, 1011, 4664],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 16480, 2571],\n",
              " [30522, 1011, 4895, 5562, 2595, 5723],\n",
              " [30525, 1011, 8810],\n",
              " [30524, 1011, 3095],\n",
              " [30533, 1011, 5785, 3927],\n",
              " [30523, 1011, 4203, 10768, 24881, 16521],\n",
              " [30524, 1011, 15610],\n",
              " [30528, 1011, 2264, 2352, 10733],\n",
              " [30524, 1011, 15610],\n",
              " [30524, 1011, 15610, 2015],\n",
              " [30522, 1011, 2543, 2173],\n",
              " [30523, 1011, 9388, 5603, 11124, 2696, 10733],\n",
              " [30530, 1011, 9841],\n",
              " [30524, 1011, 3095],\n",
              " [30523, 1011, 21480, 2015],\n",
              " [30522, 1011, 4825],\n",
              " [30522, 1011, 5023, 28942],\n",
              " [30523, 1011, 20130, 10439, 20624, 16750],\n",
              " [30523, 1011, 23726, 2078],\n",
              " [30524, 1011, 7505, 7903, 2063],\n",
              " [30523, 1011, 14732, 14704],\n",
              " [30522, 1011, 7251],\n",
              " [30523, 1011, 2796, 2833],\n",
              " [30530, 1011, 6770, 2050, 7852],\n",
              " [30528, 1011, 4825],\n",
              " [30523, 1011, 27940, 28774, 8717, 3211],\n",
              " [30527, 1011, 10733, 2173],\n",
              " [30528, 1011, 4825],\n",
              " [30524, 1011, 2326],\n",
              " [30525, 1011, 10439, 20624, 6290, 4989],\n",
              " [30523, 1011, 2796, 2833],\n",
              " [30528, 1011, 2173],\n",
              " [30524, 1011, 3208],\n",
              " [30527, 1011, 2173],\n",
              " [30522, 1011, 4272],\n",
              " [30525, 1011, 3529, 2015, 2005, 2364, 4372, 13334],\n",
              " [30528, 1011, 2173],\n",
              " [30530, 1011, 7975, 14841, 15714, 16137, 7911],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 6151, 2050, 1006, 8288, 1007, 9372],\n",
              " [30532, 1011, 4511, 2862, 4989],\n",
              " [30522, 1011, 2572, 11283, 5897],\n",
              " [30528, 1011, 2173],\n",
              " [30524, 1011, 15610],\n",
              " [30523, 1011, 3869],\n",
              " [30528, 1011, 26366, 3962],\n",
              " [30524, 1011, 10170],\n",
              " [30523, 1011, 3419, 1047, 3270, 11350],\n",
              " [30528, 1011, 2715, 2887, 8782, 17378],\n",
              " [30524, 1011, 2326],\n",
              " [30522, 1011, 7224],\n",
              " [30523, 1011, 19739, 19629, 5302, 2571],\n",
              " [30523, 1011, 11687, 7367, 1041, 2860, 7975],\n",
              " [30523, 1011, 7954],\n",
              " [30523, 1011, 8292, 12933, 2063, 4666, 1006, 2569, 1007],\n",
              " [30530, 1011, 2833],\n",
              " [30522, 1011, 21209],\n",
              " [30523, 1011, 23621],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 10733],\n",
              " [30523, 1011, 6077],\n",
              " [30528, 1011, 2173],\n",
              " [30525, 1011, 2833],\n",
              " [30526, 1011, 11015, 1997, 4511],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 24799],\n",
              " [30524, 1011, 3208],\n",
              " [30522, 1011, 2173],\n",
              " [30523, 1011, 24857, 2015],\n",
              " [30523, 1011, 5703, 1997, 17561, 1998, 12851],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 3095],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 2968],\n",
              " [30528, 1011, 2173],\n",
              " [30528, 1011, 8038, 18900, 2080],\n",
              " [30523, 1011, 2833],\n",
              " [30528, 1011, 4825],\n",
              " [30526, 1011, 14746],\n",
              " [30532, 1011, 11831, 5404],\n",
              " [30523, 1011, 15594, 2100, 9457],\n",
              " [30527, 1011, 2796, 2173],\n",
              " [30524, 1011, 15610],\n",
              " [30523, 1011, 9587, 16308],\n",
              " [30525, 1011, 12183],\n",
              " [30523, 1011, 14841, 6444, 2483, 2226],\n",
              " [30524, 1011, 2326],\n",
              " [30530, 1011, 20323],\n",
              " [30523, 1011, 12760],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 10439, 20624, 16750],\n",
              " [30523, 1011, 11687, 7273],\n",
              " [30523, 1011, 10733],\n",
              " [30523, 1011, 9841],\n",
              " [30526, 1011, 24480],\n",
              " [30523, 1011, 12760],\n",
              " [30523, 1011, 17294, 11642],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 7954],\n",
              " [30523, 1011, 2833],\n",
              " [30528, 1011, 2173],\n",
              " [30524, 1011, 22566],\n",
              " [30528, 1011, 15333, 4801, 3363, 1998, 11804, 9047],\n",
              " [30523, 1011, 11687, 21812],\n",
              " [30528, 1011, 4825],\n",
              " [30524, 1011, 2611],\n",
              " [30524, 1011, 2326],\n",
              " [30531, 1011, 3295],\n",
              " [30523, 1011, 2712, 3321],\n",
              " [30524, 1011, 13877],\n",
              " [30523, 1011, 16130, 14744, 2630, 18081],\n",
              " [30523, 1011, 2600, 20130, 8915, 8737, 4648],\n",
              " [30525, 1011, 5431, 8081, 2063, 12183],\n",
              " [30523, 1011, 23621, 26666],\n",
              " [30523, 1011, 9587, 20715, 21835, 4372, 2482, 18153, 4143],\n",
              " [30522, 1011, 7224],\n",
              " [30525, 1011, 12183],\n",
              " [30524, 1011, 3095],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30532, 1011, 4511, 1011, 2011, 1011, 1996, 1011, 3221],\n",
              " [30523, 1011, 20209, 4569],\n",
              " [30523, 1011, 9889, 2833],\n",
              " [30524, 1011, 3208],\n",
              " [30522, 1011, 2572, 11283, 5897],\n",
              " [30523, 1011, 2962, 4605],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 2173],\n",
              " [30527, 1011, 2173],\n",
              " [30525, 1011, 8810],\n",
              " [30522, 1011, 8855, 3536, 25545],\n",
              " [30525, 1011, 10026, 2546],\n",
              " [30523, 1011, 10733],\n",
              " [30522, 1011, 2173],\n",
              " [30522, 1011, 25545],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 4166, 6829],\n",
              " [30522, 1011, 21209],\n",
              " [30527, 1011, 1996, 2176, 3692],\n",
              " [30528, 1011, 4825],\n",
              " [30528, 1011, 4825],\n",
              " [30530, 1011, 22286, 2015],\n",
              " [30522, 1011, 7224],\n",
              " [30523, 1011, 12559, 10447],\n",
              " [30523, 1011, 8840, 2595],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 2665, 5572, 8915, 8737, 4648],\n",
              " [30523, 1011, 2796, 2822],\n",
              " [30523, 1011, 7759],\n",
              " [30524, 1011, 8013, 2326],\n",
              " [30524, 1011, 3954],\n",
              " [30523, 1011, 11424, 1011, 2822, 2833],\n",
              " [30525, 1011, 10861, 3736, 4305, 4571],\n",
              " [30528, 1011, 13426, 1005, 1055],\n",
              " [30522, 1011, 7224],\n",
              " [30528, 1011, 2173],\n",
              " [30526, 1011, 4157],\n",
              " [30523, 1011, 12486, 2053, 26156, 11350],\n",
              " [30524, 1011, 2326],\n",
              " [30522, 1011, 2173],\n",
              " [30525, 1011, 3869],\n",
              " [30527, 1011, 2173],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 2326],\n",
              " [30531, 1011, 3193, 1997, 2314, 1998, 16392],\n",
              " [30523, 1011, 3869],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 8013, 2326],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 2326],\n",
              " [30531, 1011, 2173],\n",
              " [30524, 1011, 15812],\n",
              " [30524, 1011, 22566],\n",
              " [30528, 1011, 16729, 29493, 4048],\n",
              " [30523, 1011, 17488, 20934, 22510, 2050],\n",
              " [30523, 1011, 10424, 7616],\n",
              " [30524, 1011, 3095],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2312, 2878, 20130],\n",
              " [30532, 1011, 4392, 12183],\n",
              " [30523, 1011, 23621, 6415, 6632, 23567, 2063],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 10077, 21438],\n",
              " [30522, 1011, 2173],\n",
              " [30522, 1011, 25545],\n",
              " [30523, 1011, 7273, 2833],\n",
              " [30523, 1011, 10026],\n",
              " [30524, 1011, 2326],\n",
              " [30522, 1011, 2572, 15599, 3401],\n",
              " [30526, 1011, 8974],\n",
              " [30523, 1011, 24799, 1997, 11721, 3089],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 2796, 2833],\n",
              " [30523, 1011, 15653, 11227],\n",
              " [30523, 1011, 6420, 3609, 1998, 4496, 8545, 18150, 2319, 9372],\n",
              " [30532, 1011, 4511, 2862],\n",
              " [30530, 1011, 2833],\n",
              " [30524, 1011, 6959, 4364],\n",
              " [30522, 1011, 2173],\n",
              " [30523, 1011, 2833],\n",
              " [30526, 1011, 8974],\n",
              " [30523, 1011, 10733],\n",
              " [30528, 1011, 2632, 4487, 2474],\n",
              " [30523, 1011, 14163, 21218, 2015, 1999, 25482, 20856, 12901],\n",
              " [30523, 1011, 2735, 11514, 9850],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2822, 2833],\n",
              " [30523, 1011, 19116],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 15610],\n",
              " [30523, 1011, 22201],\n",
              " [30522, 1011, 3347],\n",
              " [30526, 1011, 4511],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30522, 1011, 4825],\n",
              " [30523, 1011, 2833],\n",
              " [30527, 1011, 4825],\n",
              " [30526, 1011, 8974],\n",
              " [30530, 1011, 2833],\n",
              " [30524, 1011, 3095],\n",
              " [30524, 1011, 14903],\n",
              " [30526, 1011, 8739],\n",
              " [30523, 1011, 2364, 2607],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 6714, 6776],\n",
              " [30528, 1011, 4825],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30526, 1011, 24480, 2015],\n",
              " [30524, 1011, 15610],\n",
              " [30523, 1011, 3869, 11937, 13186],\n",
              " [30523, 1011, 26192, 2094, 5003, 4048, 5003, 4048],\n",
              " [30528, 1011, 2417, 3239, 18651],\n",
              " [30523, 1011, 22286, 2015],\n",
              " [30526,\n",
              "  1011,\n",
              "  18901,\n",
              "  2007,\n",
              "  20418,\n",
              "  21092,\n",
              "  1998,\n",
              "  14380,\n",
              "  1998,\n",
              "  14123,\n",
              "  10869,\n",
              "  1998,\n",
              "  12927,\n",
              "  3727],\n",
              " [30523, 1011, 8808, 5127],\n",
              " [30524, 1011, 2326],\n",
              " [30525, 1011, 2833],\n",
              " [30523, 1011, 2627, 6444, 2072],\n",
              " [30524, 1011, 3095],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 15544, 6499, 9284, 2015],\n",
              " [30522, 1011, 2067, 3871, 3564, 2181],\n",
              " [30523, 1011, 12901, 2015],\n",
              " [30527, 1011, 2173],\n",
              " [30524, 1011, 3095],\n",
              " [30523, 1011, 2833],\n",
              " [30528, 1011, 2173],\n",
              " [30522, 1011, 7224],\n",
              " [30530, 1011, 2833],\n",
              " [30523, 1011, 11350, 2005, 1996, 20904, 2239],\n",
              " [30523, 1011, 2833],\n",
              " [30528, 1011, 4825],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 24511, 27605],\n",
              " [30526, 1011, 12348, 5404],\n",
              " [30523, 1011, 2833],\n",
              " [30528, 1011, 10733, 3943],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 5532],\n",
              " [30523, 1011, 2980, 6077],\n",
              " [30524, 1011, 6754],\n",
              " [30531, 1011, 3193],\n",
              " [30525, 1011, 18064],\n",
              " [30533, 1011, 11286],\n",
              " [30524, 1011, 2326],\n",
              " [30522, 1011, 4044],\n",
              " [30524, 1011, 3208],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2413, 20949, 11350],\n",
              " [30525, 1011, 4989, 1997, 4857, 19116, 10733],\n",
              " [30523, 1011, 4372, 13334],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 6187, 9035, 2099],\n",
              " [30523, 1011, 12901],\n",
              " [30530, 1011, 16539, 2833],\n",
              " [30523, 1011, 17823, 5856, 1011, 8945, 5283, 2213],\n",
              " [30524, 1011, 15610],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 10747],\n",
              " [30523, 1011, 12901],\n",
              " [30523,\n",
              "  1011,\n",
              "  2004,\n",
              "  28689,\n",
              "  12349,\n",
              "  1010,\n",
              "  19817,\n",
              "  16093,\n",
              "  21031,\n",
              "  3514,\n",
              "  1010,\n",
              "  11968,\n",
              "  7834,\n",
              "  2319,\n",
              "  7987,\n",
              "  2271,\n",
              "  20318,\n",
              "  2696],\n",
              " [30523, 1011, 2833],\n",
              " [30532, 1011, 5404],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 23566, 9841],\n",
              " [30522, 1011, 2173],\n",
              " [30527, 1011, 4306],\n",
              " [30524, 1011, 3095],\n",
              " [30524, 1011, 2326],\n",
              " [30530, 1011, 5785, 10447],\n",
              " [30523, 1011, 10250, 8067, 3089],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 2715, 2887, 8782, 17378],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 15960, 13675, 2080, 29416, 11642],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 3095],\n",
              " [30525, 1011, 6420, 3609, 1998, 4496, 8545, 18150, 2319, 9372],\n",
              " [30529, 1011, 4511],\n",
              " [30533, 1011, 2173],\n",
              " [30528, 1011, 4825],\n",
              " [30527, 1011, 3534, 2080],\n",
              " [30523, 1011, 5431, 2063, 8081, 2063, 18767, 12183],\n",
              " [30522, 1011, 13536],\n",
              " [30522, 1011, 22243, 3681],\n",
              " [30523, 1011, 2980, 12901],\n",
              " [30523, 1011, 2474, 9508],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 4166, 3347],\n",
              " [30522, 1011, 19135, 3347],\n",
              " [30528, 1011, 10975, 9816],\n",
              " [30531, 1011, 3295],\n",
              " [30523, 1011, 6240],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 18834],\n",
              " [30523, 1011, 2833],\n",
              " [30530, 1011, 23621],\n",
              " [30522, 1011, 25545],\n",
              " [30523, 1011, 18064],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 10447],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 7273, 2833],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 2833],\n",
              " [30532, 1011, 18007],\n",
              " [30522, 1011, 2273, 2015, 5723],\n",
              " [30523, 1011, 5371, 2102],\n",
              " [30526, 1011, 3347, 8974],\n",
              " [30531, 1011, 5328],\n",
              " [30523, 1011, 12278],\n",
              " [30525, 1011, 2217, 10447],\n",
              " [30523, 1011, 11424, 2822, 2833],\n",
              " [30524, 1011, 3524, 1011, 3095],\n",
              " [30523, 1011, 3306, 1998, 18543, 10447],\n",
              " [30523, 1011, 2833],\n",
              " [30531, 1011, 3962],\n",
              " [30523, 1011, 18081, 4168, 4017, 5869, 8490, 2532],\n",
              " [30523, 1011, 10447],\n",
              " [30523, 1011, 2980, 6077],\n",
              " [30530, 1011, 2833],\n",
              " [30522, 1011, 3564, 2686],\n",
              " [30525, 1011, 12183],\n",
              " [30522, 1011, 5723],\n",
              " [30523, 1011, 2980, 6077],\n",
              " [30523, 1011, 6763, 12122],\n",
              " [30530, 1011, 27130],\n",
              " [30530, 1011, 2833],\n",
              " [30523, 1011, 7273, 2833],\n",
              " [30523, 1011, 2887, 2833],\n",
              " [30526, 1011, 8739],\n",
              " [30523, 1011, 10733],\n",
              " [30524, 1011, 2326],\n",
              " [30528, 1011, 11286],\n",
              " [30533, 1011, 2173],\n",
              " [30526, 1011, 14746],\n",
              " [30527, 1011, 5751],\n",
              " [30524, 1011, 15812],\n",
              " [30522, 1011, 2173],\n",
              " [30523, 1011, 4078, 18116, 3215],\n",
              " [30523, 1011, 9372],\n",
              " [30530, 1011, 19739, 19629, 5302, 2571],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 2202, 5833],\n",
              " [30524, 1011, 8241],\n",
              " [30524, 1011, 3829],\n",
              " [30523, 1011, 18672, 2072, 9372],\n",
              " [30525, 1011, 5785, 2000, 3869, 6463, 2078],\n",
              " [30522, 1011, 25545],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 2887, 11112, 3022],\n",
              " [30523, 1011, 12559, 2569],\n",
              " [30524, 1011, 3524, 3095],\n",
              " [30530, 1011, 12183],\n",
              " [30525, 1011, 15890, 2015],\n",
              " [30523, 1011, 15212, 8915, 8737, 4648],\n",
              " [30523, 1011, 16130, 1011, 13017, 26852],\n",
              " [30523, 1011, 2833],\n",
              " [30529, 1011, 4511, 2862],\n",
              " [30525, 1011, 3424, 19707, 3775],\n",
              " [30529, 1011, 4511, 4989],\n",
              " [30523, 1011, 12205, 21475],\n",
              " [30523, 1011, 13697, 9850, 18064],\n",
              " [30522, 1011, 7254, 10747],\n",
              " [30528, 1011, 2173],\n",
              " [30524, 1011, 15610],\n",
              " [30524, 1011, 2111],\n",
              " [30532, 1011, 5761, 1997, 5622, 4226, 2869],\n",
              " [30523, 1011, 2980, 3899],\n",
              " [30523, 1011, 27940, 11642],\n",
              " [30523, 1011, 5371, 2102, 19117, 8540, 9841],\n",
              " [30523, 1011, 7975],\n",
              " [30523, 1011, 28144, 2389],\n",
              " [30530, 1011, 5035, 5428],\n",
              " [30524, 1011, 2326],\n",
              " [30522, 1011, 8103, 4430],\n",
              " [30525, 1011, 13017, 21480, 2015, 1998, 18856, 13596],\n",
              " [30523, 1011, 10733],\n",
              " [30532, 1011, 18007],\n",
              " [30523, 1011, 13017, 15653, 11227],\n",
              " [30522, 1011, 2522, 21678, 2063],\n",
              " [30524, 1011, 2326],\n",
              " [30532, 1011, 4511, 2862],\n",
              " [30523, 1011, 10733],\n",
              " [30523, 1011, 2833],\n",
              " [30525, 1011, 8810],\n",
              " [30523, 1011, 3899],\n",
              " [30524, 1011, 2111],\n",
              " [30528, 1011, 2717, 5400, 16671],\n",
              " [30522, 1011, 3871, 11134],\n",
              " [30524, 1011, 13877],\n",
              " [30533, 1011, 2717, 4648, 3372],\n",
              " [30525, 1011, 2833],\n",
              " [30527, 1011, 3962],\n",
              " [30531, 1011, 5785, 3927],\n",
              " [30523, 1011, 2833],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 11350, 2100, 15653, 11227],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 7638],\n",
              " [30528, 1011, 2771, 9759],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 11134],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 3829, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 9665, 10733],\n",
              " [30528, 1011, 2173],\n",
              " [30527, 1011, 3962],\n",
              " [30530, 1011, 10447],\n",
              " [30532, 1011, 8739, 2862],\n",
              " [30524, 1011, 3524, 3095],\n",
              " [30525, 1011, 4989],\n",
              " [30523, 1011, 2521, 3217, 16521],\n",
              " [30524, 1011, 13877],\n",
              " [30523, 1011, 24511, 27605],\n",
              " [30524, 1011, 8241],\n",
              " [30524, 1011, 3524],\n",
              " [30523, 1011, 16521, 2015],\n",
              " [30528, 1011, 13426, 1005, 1055],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 7224],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 24444],\n",
              " [30523, 1011, 19247],\n",
              " [30528, 1011, 2173],\n",
              " [30524, 1011, 3095],\n",
              " [30522, 1011, 6520],\n",
              " [30522, 1011, 2067, 2282],\n",
              " [30528, 1011, 2173],\n",
              " [30528, 1011, 2771, 9759],\n",
              " [30523, 1011, 2822, 2806, 2796, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30522, 1011, 2514],\n",
              " [30523, 1011, 11565, 10698],\n",
              " [30523, 1011, 2833],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 2833],\n",
              " [30527, 1011, 2173],\n",
              " [30524, 1011, 2326],\n",
              " [30525, 1011, 4524, 2884],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 25545],\n",
              " [30522, 1011, 4734],\n",
              " [30524, 1011, 3095],\n",
              " [30522, 1011, 3494],\n",
              " [30523, 1011, 14557, 7395],\n",
              " [30522, 1011, 7759, 2282],\n",
              " [30529, 1011, 8974],\n",
              " [30527, 1011, 2173],\n",
              " [30528, 1011, 3534, 2080],\n",
              " [30523, 1011, 3147, 10439, 20624, 6290, 10447],\n",
              " [30522, 1011, 2119, 2015],\n",
              " [30523, 1011, 2217, 10447],\n",
              " [30522, 1011, 4825],\n",
              " [30523, 1011, 9372],\n",
              " [30524, 1011, 15610],\n",
              " [30523, 1011, 2833],\n",
              " [30529, 1011, 8974],\n",
              " [30522, 1011, 25779, 4799, 4303],\n",
              " [30524, 1011, 25914],\n",
              " [30523, 1011, 2833],\n",
              " [30533, 1011, 4825],\n",
              " [30523, 1011, 26192, 2094, 11840, 9841],\n",
              " [30525, 1011, 9372],\n",
              " [30522, 1011, 2173],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 7954],\n",
              " [30523, 1011, 12760],\n",
              " [30530, 1011, 10733, 2015],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30530, 1011, 2887, 2833],\n",
              " [30523, 1011, 2176, 2607, 5431, 8081, 12183],\n",
              " [30526, 1011, 4511],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 4759, 2833],\n",
              " [30528, 1011, 7668, 15587],\n",
              " [30523, 1011, 10556, 3676, 5910],\n",
              " [30523, 1011, 2833],\n",
              " [30530, 1011, 2833],\n",
              " [30525, 1011, 4664],\n",
              " [30527, 1011, 3083, 13642, 3962],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 4524, 9050],\n",
              " [30523, 1011, 3180, 12183, 1011, 13258],\n",
              " [30523, 1011, 2980, 3899],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30528, 1011, 4825],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 3869, 10447],\n",
              " [30524, 1011, 3095],\n",
              " [30532, 1011, 8739, 12183],\n",
              " [30523, 1011, 2833],\n",
              " [30525, 1011, 2627, 6444, 2072, 11642, 2006, 1037, 4897],\n",
              " [30528, 1011, 10514, 2319],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 3347, 3496],\n",
              " [30522, 1011, 2173],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 20861],\n",
              " [30523, 1011, 5035, 25923],\n",
              " [30522, 1011, 3496],\n",
              " [30528, 1011, 3869],\n",
              " [30531, 1011, 3295],\n",
              " [30528, 1011, 2173],\n",
              " [30530, 1011, 2833],\n",
              " [30522, 1011, 25545],\n",
              " [30523, 1011, 2175, 2175, 24575, 2015],\n",
              " [30528, 1011, 21887],\n",
              " [30528, 1011, 11941],\n",
              " [30528, 1011, 4825],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30523, 1011, 2796],\n",
              " [30522, 1011, 2173],\n",
              " [30523, 1011, 9841],\n",
              " [30524, 1011, 15610, 2015],\n",
              " [30532, 1011, 4511, 2862],\n",
              " [30523, 1011, 9841],\n",
              " [30524, 1011, 15610, 2015],\n",
              " [30525, 1011, 2980, 3899],\n",
              " [30528, 1011, 14412, 3401],\n",
              " [30523, 1011, 2833],\n",
              " [30533, 1011, 10514, 2319],\n",
              " [30523, 1011, 2887, 7216, 2833],\n",
              " [30530, 1011, 22094],\n",
              " [30524, 1011, 14903],\n",
              " [30525, 1011, 10447],\n",
              " [30523, 1011, 10733],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 7852],\n",
              " [30525, 1011, 2833],\n",
              " [30525, 1011, 12183],\n",
              " [30525, 1011, 12183],\n",
              " [30525, 1011, 8810],\n",
              " [30530, 1011, 6265, 28305],\n",
              " [30523, 1011, 10447],\n",
              " [30523, 1011, 2833],\n",
              " [30522, 1011, 7759, 3871],\n",
              " [30525, 1011, 21475, 16985, 7559, 2063],\n",
              " [30523, 1011, 22861, 4160, 11840],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 15544, 6499, 9284],\n",
              " [30523, 1011, 17776, 18856, 13596, 24318],\n",
              " [30523, 1011, 12559, 24165, 2015],\n",
              " [30528, 1011, 16897],\n",
              " [30522, 1011, 7224],\n",
              " [30524, 1011, 6959],\n",
              " [30528, 1011, 2502, 11789],\n",
              " [30523, 1011, 26192, 2094, 7975, 2569, 2007, 3968, 8067, 4168, 5760, 2063],\n",
              " [30523, 1011, 10447],\n",
              " [30523, 1011, 9372],\n",
              " [30523, 1011, 10720, 2072, 7975],\n",
              " [30532, 1011, 4511],\n",
              " [30528, 1011, 4825],\n",
              " [30522, 1011, 2572, 15599, 3401],\n",
              " [30530, 1011, 8808, 5127],\n",
              " [30528, 1011, 1996, 2176, 3692],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30523, 1011, 2712, 3321],\n",
              " [30524, 1011, 2326],\n",
              " [30526, 1011, 4511],\n",
              " [30523, 1011, 3846, 26348, 2015, 2058, 5785],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 16521, 2015],\n",
              " [30523, 1011, 16539, 2833],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 18565, 10733],\n",
              " [30522, 1011, 6506],\n",
              " [30523, 1011, 9388, 5603, 11124, 2696, 10733],\n",
              " [30522, 1011, 7224],\n",
              " [30522, 1011, 7579, 10487],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30523, 1011, 3869, 1998, 11772],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 18064],\n",
              " [30523, 1011, 14163, 4757, 4244],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 2417, 5202, 4897],\n",
              " [30524, 1011, 10747],\n",
              " [30522, 1011, 25545],\n",
              " [30525, 1011, 3167, 6090, 2015],\n",
              " [30523, 1011, 6187, 9035, 2099],\n",
              " [30524, 1011, 2326],\n",
              " [30524, 1011, 8013, 2326],\n",
              " [30523, 1011, 19802, 2401],\n",
              " [30523, 1011, 7954],\n",
              " [30525, 1011, 2431, 1013, 2431, 10733],\n",
              " [30522, 1011, 3542, 4887],\n",
              " [30522, 1011, 4044],\n",
              " [30523, 1011, 24511, 27605, 2572, 8557, 8945, 19140],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 2715, 2887, 2833],\n",
              " [30528, 1011, 19817, 19321, 11069],\n",
              " [30523, 1011, 7975, 1999, 1996, 16521, 2015],\n",
              " [30522, 1011, 25545],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 10026, 1005, 1055, 19247],\n",
              " [30524, 1011, 6959],\n",
              " [30523, 1011, 2001, 16336, 14629],\n",
              " [30523, 1011, 2310, 2389],\n",
              " [30525, 1011, 2833],\n",
              " [30523, 1011, 2833],\n",
              " [30524, 1011, 13877],\n",
              " [30524, 1011, 2326],\n",
              " [30530, 1011, 4596],\n",
              " [30523, 1011, 2833],\n",
              " [30525, 1011, 27940, 13513],\n",
              " [30524, 1011, 26706],\n",
              " [30530, 1011, 10733],\n",
              " [30523, 1011, 7975, 8840, 6894, 16340],\n",
              " [30530, 1011, 2833],\n",
              " [30528, 1011, 10733, 2173],\n",
              " [30528, 1011, 2173],\n",
              " [30523, 1011, 21122],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 10733, 2015],\n",
              " [30522, 1011, 2717, 4648, 3372],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 12760],\n",
              " [30524, 1011, 2326],\n",
              " [30528, 1011, 4825],\n",
              " [30522, 1011, 2173],\n",
              " [30523, 1011, 18081, 15653, 11227],\n",
              " [30528, 1011, 4825],\n",
              " [30523, 1011, 18906, 10672, 2015, 2007, 27529],\n",
              " [30524, 1011, 2326],\n",
              " [30523, 1011, 2833],\n",
              " [30523, 1011, 10514, 6182],\n",
              " [30522, 1011, 7224],\n",
              " [30524, 1011, 2326],\n",
              " [30528, 1011, 2173],\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goZO7KG-3VaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "labels = encoder.fit_transform(df.polarity.values)\n",
        "labels_dev = encoder.transform(df_dev.polarity.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8exB9d1FrfpY",
        "colab_type": "code",
        "outputId": "fbc28693-22f9-4111-d48d-aee10d23f644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(max(first_input_ids, key=len)) + len(max(auxiliary_input_ids, key=len))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "110"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhOYAbDQGNgS",
        "colab_type": "code",
        "outputId": "0aa2be7e-d157-475a-eb6a-baaae2863da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(max(first_input_ids_dev, key=len)) + len(max(auxiliary_input_ids_dev, key=len))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szDPxL3Kt1fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAz_P3B9rLpI",
        "colab_type": "code",
        "outputId": "1c14554e-ab6b-4cc9-f525-1f690a557219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "def prepare_input_ids(first_input_ids, auxiliary_input_ids):\n",
        "  input_ids_prepared = [tokenizer.\n",
        "              prepare_for_model(\n",
        "                  input_ids_0, \n",
        "                  input_ids_1,\n",
        "                  max_length=max_length,\n",
        "                  truncation_strategy='only_first', \n",
        "                  pad_to_max_length=True, \n",
        "                  return_token_type_ids=True,\n",
        "                  return_attention_mask=True) for \n",
        "              input_ids_0, input_ids_1 in \n",
        "              list(zip(first_input_ids, auxiliary_input_ids))]\n",
        "\n",
        "  df_input_ids_prepared = pd.DataFrame(input_ids_prepared)\n",
        "\n",
        "  input_ids = list(df_input_ids_prepared.input_ids.values)\n",
        "  token_type_ids = list(df_input_ids_prepared.token_type_ids.values)\n",
        "  attention_masks = list(df_input_ids_prepared.attention_mask.values)\n",
        "\n",
        "  print('input_ids',input_ids[0])\n",
        "  print('token_type_ids',token_type_ids[0])\n",
        "  print('attention_mask',attention_masks[0])\n",
        "\n",
        "  return input_ids, token_type_ids, attention_masks\n",
        "\n",
        "input_ids, token_type_ids, attention_masks = prepare_input_ids(\n",
        "    first_input_ids, auxiliary_input_ids)\n",
        "\n",
        "input_ids_dev, token_type_ids_dev, attention_masks_dev = prepare_input_ids(\n",
        "    first_input_ids_dev, auxiliary_input_ids_dev)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_ids [101, 2023, 24209, 22325, 1998, 6298, 19817, 19321, 11069, 2003, 2012, 1996, 2327, 1997, 2026, 7128, 4825, 2862, 1012, 102, 30522, 1011, 19817, 19321, 11069, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "token_type_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "input_ids [101, 1045, 2245, 2023, 2173, 2001, 6135, 2058, 9250, 1012, 102, 30528, 1011, 2173, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "token_type_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWu-JU4lvbrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "def convert_to_tensors(input_ids, token_type_ids, attention_masks, labels):\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  labels = torch.tensor(labels)\n",
        "  attention_masks = torch.tensor(attention_masks)\n",
        "  token_type_ids = torch.tensor(token_type_ids)\n",
        "  return input_ids, token_type_ids, attention_masks, labels\n",
        "\n",
        "input_ids, token_type_ids, attention_masks, labels = convert_to_tensors(\n",
        "    input_ids, token_type_ids, attention_masks, labels)\n",
        "\n",
        "input_ids_dev, token_type_ids_dev, attention_masks_dev, labels_dev = convert_to_tensors(\n",
        "    input_ids_dev, token_type_ids_dev, attention_masks_dev, labels_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqnYXAUYxj4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_data_loader(input_ids, token_type_ids, attention_masks, labels, batch_size=32):\n",
        "  data = TensorDataset(input_ids, token_type_ids, attention_masks, labels)\n",
        "  sampler = RandomSampler(data)\n",
        "  dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "  return dataloader\n",
        "\n",
        "dataloader = create_data_loader(\n",
        "    input_ids, token_type_ids, attention_masks, labels)\n",
        "\n",
        "dataloader_dev = create_data_loader(\n",
        "    input_ids_dev, token_type_ids_dev, attention_masks_dev, labels_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2V7GHwgxwdQ",
        "colab_type": "code",
        "outputId": "75ff5bfb-cf8b-4732-884f-626baddf875d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.cuda()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30534, 768)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju9WRqe_7Qbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK1Q8GxS8WvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(params = model.parameters(), lr=1.5e-5, weight_decay=0.00)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMO9xbZS8X7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FToO4ehw9kJD",
        "colab_type": "code",
        "outputId": "6b1474fd-29ad-45f0-a8e1-2385cd7ceb03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "t = [] \n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_token_type_ids, b_attention_masks, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss, logits = model(b_input_ids, token_type_ids = b_token_type_ids, attention_mask = b_attention_masks, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in dataloader_dev:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_token_type_ids, b_attention_masks, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      (loss, logits) = model(b_input_ids, token_type_ids = b_token_type_ids, attention_mask = b_attention_masks, labels=b_labels) \n",
        "  \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.77819147769441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  25%|██▌       | 1/4 [00:33<01:41, 33.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7647192028985508\n",
            "Train loss: 0.453491322537686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 2/4 [01:07<01:07, 33.64s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8370697463768115\n",
            "Train loss: 0.31842512050841715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  75%|███████▌  | 3/4 [01:40<00:33, 33.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8464673913043478\n",
            "Train loss: 0.2481942484353451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 4/4 [02:13<00:00, 33.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8516757246376812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}