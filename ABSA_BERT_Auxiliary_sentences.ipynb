{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABSA-BERT-Auxiliary-sentences.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnxVpAuBnc3Q",
        "colab_type": "code",
        "outputId": "978bf3f2-4eec-481c-a0d7-a7f59fa49700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpTIr7CpoZrP",
        "colab_type": "code",
        "outputId": "503c6580-e966-495a-95cb-c77860c1a24a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.12.39)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.38.0)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.15.39)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.39->boto3->pytorch-pretrained-bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFMkICw7ocT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5iwC4LDoevm",
        "colab_type": "code",
        "outputId": "37a6082e-199e-40a0-8624-b6f6fa191d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcd2Fif3pqcw",
        "colab_type": "code",
        "outputId": "ea21919a-c36b-43e5-e4bb-9b0165de7a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "df = pd.read_csv('data/traindata.csv', sep='\\t', header=0, names=['polarity', 'aspect', 'target', 'position', 'sentence'])\n",
        "df.head(3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>aspect</th>\n",
              "      <th>target</th>\n",
              "      <th>position</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>AMBIENCE#GENERAL</td>\n",
              "      <td>trattoria</td>\n",
              "      <td>25:34</td>\n",
              "      <td>This quaint and romantic trattoria is at the t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>food</td>\n",
              "      <td>98:102</td>\n",
              "      <td>The have over 100 different beers to offer thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>STAFF</td>\n",
              "      <td>5:10</td>\n",
              "      <td>THIS STAFF SHOULD BE FIRED.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                           sentence\n",
              "0  positive  ...  This quaint and romantic trattoria is at the t...\n",
              "1  positive  ...  The have over 100 different beers to offer thi...\n",
              "2  negative  ...                        THIS STAFF SHOULD BE FIRED.\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFMgNAf4qA2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "encoder = LabelEncoder()\n",
        "labels = encoder.fit_transform(df.polarity.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efUluKklwmoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oMuPp_eqj2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bf0e397e-fbed-4ac0-deac-d88699941d6b"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[2])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1962363.88B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'this', 'staff', 'should', 'be', 'fired', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8exB9d1FrfpY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32c3b79c-eff9-4097-c108-df3baabd4afd"
      },
      "source": [
        "len(max(tokenized_texts, key=len))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szDPxL3Kt1fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNonddQEuRjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cWqDgS_uSj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72gUQdXjunMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create segment masks\n",
        "segment_masks = []\n",
        "\n",
        "for seq in input_ids:\n",
        "  seq_mask = [0 for i in seq]\n",
        "  segment_masks.append(seq_mask)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwRdxHECueMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOTDIejHvYRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_attention_masks, validation_attention_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "train_segment_masks, validation_segment_masks, _, _ = train_test_split(segment_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWu-JU4lvbrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_attention_masks = torch.tensor(train_attention_masks)\n",
        "validation_attention_masks = torch.tensor(validation_attention_masks)\n",
        "\n",
        "train_segment_masks = torch.tensor(train_segment_masks)\n",
        "validation_segment_masks = torch.tensor(validation_segment_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqnYXAUYxj4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_attention_masks, train_segment_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_attention_masks, validation_segment_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2V7GHwgxwdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8539771c-42ad-42b2-d310-f3382dcfe50a"
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "model.cuda()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:05<00:00, 75142559.40B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju9WRqe_7Qbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK1Q8GxS8WvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54e0d6ff-ad23-4b74-b777-b8e95d397ba0"
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t_total value of -1 results in schedule not being applied\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMO9xbZS8X7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FToO4ehw9kJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "853d785e-1fdc-4d7b-e84b-dcae5343ced3"
      },
      "source": [
        "t = [] \n",
        "\n",
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_attention_mask, b_input_segment_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    print(type(b_input_ids))\n",
        "    loss = model(b_input_ids, token_type_ids = b_input_segment_mask, attention_mask = b_input_attention_mask, labels=b_labels)\n",
        "    print(loss)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_attention_mask, b_input_segment_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids = b_input_segment_mask, attention_mask = b_input_attention_mask)  \n",
        "\n",
        "  \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.5226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Train loss: 0.1424780154491172\n",
            "b_input_ids tensor([[  101,  1996, 13877,  ...,     0,     0,     0],\n",
            "        [  101,  2061,  2054,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2326,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2123,  1005,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2069,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0], device='cuda:0')\n",
            "logits tensor([[ 2.5594, -1.8499, -0.9600],\n",
            "        [ 2.8969, -1.6581, -1.3129],\n",
            "        [-2.5102, -1.9239,  4.3559],\n",
            "        [-2.7035, -1.8362,  4.4712],\n",
            "        [-2.4915, -2.0652,  4.4377],\n",
            "        [-2.3191, -1.8489,  4.1074],\n",
            "        [-2.2696, -2.1602,  4.3309],\n",
            "        [ 4.0431, -1.6182, -2.5526],\n",
            "        [-2.6257, -1.4917,  4.1723],\n",
            "        [-2.6667, -1.9167,  4.4193],\n",
            "        [-2.5341, -2.0371,  4.3711],\n",
            "        [ 0.4769,  1.0427, -1.1315],\n",
            "        [-1.3534, -1.5991,  2.9295],\n",
            "        [ 1.7483, -1.3699, -0.4357],\n",
            "        [-1.4799, -2.2096,  3.4448],\n",
            "        [ 0.4262, -0.3170,  0.1242]], device='cuda:0')\n",
            "logits [[ 2.5594487  -1.8498511  -0.9600103 ]\n",
            " [ 2.896903   -1.6581291  -1.3128752 ]\n",
            " [-2.510225   -1.9238932   4.355939  ]\n",
            " [-2.703485   -1.8362272   4.4711676 ]\n",
            " [-2.491485   -2.065174    4.4377084 ]\n",
            " [-2.3191028  -1.8489327   4.1073985 ]\n",
            " [-2.2695503  -2.1602294   4.3309236 ]\n",
            " [ 4.0430603  -1.618236   -2.5525723 ]\n",
            " [-2.6257064  -1.4917159   4.17227   ]\n",
            " [-2.6667216  -1.9167482   4.419349  ]\n",
            " [-2.5341315  -2.0371096   4.371079  ]\n",
            " [ 0.4769167   1.0426513  -1.1314684 ]\n",
            " [-1.3534029  -1.5991153   2.9294631 ]\n",
            " [ 1.7483385  -1.3698559  -0.43571693]\n",
            " [-1.479929   -2.2095926   3.4447634 ]\n",
            " [ 0.42617393 -0.3169744   0.12416253]]\n",
            "label_ids [0 1 2 2 2 2 2 0 2 2 2 2 2 2 2 0]\n",
            "b_input_ids tensor([[  101, 13558, 18064,  ...,     0,     0,     0],\n",
            "        [  101,  1037,  2367,  ...,     0,     0,     0],\n",
            "        [  101,  2296,  2051,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  2173,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2428,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 18499,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 0, 2, 1, 2, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[ 2.1569,  0.4847, -2.3006],\n",
            "        [-2.4999, -1.7549,  4.1527],\n",
            "        [-2.3241, -2.0777,  4.2852],\n",
            "        [-0.7843,  3.3113, -1.7315],\n",
            "        [-2.7160, -1.7736,  4.4444],\n",
            "        [-0.8465, -1.1797,  1.8929],\n",
            "        [-0.4902,  3.4035, -2.0667],\n",
            "        [ 2.1567, -1.6911, -0.5718],\n",
            "        [-2.6991, -1.9050,  4.4369],\n",
            "        [ 0.5713, -0.7191,  0.2726],\n",
            "        [ 2.5827, -2.0807, -0.8609],\n",
            "        [-2.6505, -1.7817,  4.2673],\n",
            "        [-2.6059, -1.8761,  4.3189],\n",
            "        [-2.6336, -1.7135,  4.3214],\n",
            "        [-2.6134, -1.9785,  4.4289],\n",
            "        [ 2.5804, -2.3301, -0.7274]], device='cuda:0')\n",
            "logits [[ 2.1569238   0.48472446 -2.300637  ]\n",
            " [-2.499856   -1.7549411   4.1526604 ]\n",
            " [-2.3241017  -2.077749    4.2852473 ]\n",
            " [-0.7842622   3.3112571  -1.7314945 ]\n",
            " [-2.7160192  -1.7735821   4.444434  ]\n",
            " [-0.8464954  -1.1797241   1.8928527 ]\n",
            " [-0.49017298  3.4034944  -2.0667474 ]\n",
            " [ 2.1567464  -1.6910949  -0.57177633]\n",
            " [-2.6990905  -1.9049526   4.436867  ]\n",
            " [ 0.5712722  -0.7191477   0.27262825]\n",
            " [ 2.5826797  -2.0806901  -0.8608555 ]\n",
            " [-2.6505158  -1.7816831   4.2673097 ]\n",
            " [-2.605878   -1.8760778   4.3188934 ]\n",
            " [-2.6335897  -1.7135043   4.321438  ]\n",
            " [-2.6134157  -1.978502    4.4289246 ]\n",
            " [ 2.5804195  -2.3300595  -0.7274324 ]]\n",
            "label_ids [0 0 2 1 2 2 2 0 2 0 0 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  1045,  3811,  ...,     0,     0,     0],\n",
            "        [  101,  2025,  2069,  ...,     0,     0,     0],\n",
            "        [  101,  2017,  2442,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  4664,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 25545,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2173,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2], device='cuda:0')\n",
            "logits tensor([[-2.4787, -2.1249,  4.4107],\n",
            "        [-2.4890, -2.0772,  4.4249],\n",
            "        [-2.4286, -1.9604,  4.2326],\n",
            "        [-2.2538, -0.8872,  3.1236],\n",
            "        [-2.5416, -2.0411,  4.4453],\n",
            "        [ 2.6012, -1.7920, -0.9435],\n",
            "        [-2.8219, -1.3710,  4.2436],\n",
            "        [ 1.7427, -1.3510, -0.4464],\n",
            "        [-2.6458, -1.7712,  4.2719],\n",
            "        [-2.4530, -1.5609,  3.6261],\n",
            "        [-2.6667, -1.9167,  4.4193],\n",
            "        [-2.6850, -1.8598,  4.4610],\n",
            "        [ 3.9611, -2.0232, -2.1167],\n",
            "        [-2.8220, -1.6990,  4.4380],\n",
            "        [ 1.2050, -1.1831, -0.1380],\n",
            "        [-2.2898, -2.1484,  4.3272]], device='cuda:0')\n",
            "logits [[-2.478661   -2.1248882   4.4107165 ]\n",
            " [-2.488967   -2.0771809   4.4249225 ]\n",
            " [-2.4285827  -1.960355    4.2325897 ]\n",
            " [-2.2538254  -0.8871999   3.1235976 ]\n",
            " [-2.541624   -2.0410523   4.445282  ]\n",
            " [ 2.6012409  -1.7920419  -0.9435492 ]\n",
            " [-2.821854   -1.3709897   4.24359   ]\n",
            " [ 1.7427316  -1.3510183  -0.44641355]\n",
            " [-2.6457713  -1.7711521   4.2718625 ]\n",
            " [-2.4529793  -1.560902    3.6260657 ]\n",
            " [-2.6667216  -1.9167482   4.419349  ]\n",
            " [-2.6849952  -1.8597888   4.4610186 ]\n",
            " [ 3.9611108  -2.023162   -2.116668  ]\n",
            " [-2.8219924  -1.6989608   4.438049  ]\n",
            " [ 1.2049901  -1.1830605  -0.1379941 ]\n",
            " [-2.2897651  -2.1483526   4.327151  ]]\n",
            "label_ids [2 2 2 2 2 0 2 0 2 2 2 2 0 2 0 2]\n",
            "b_input_ids tensor([[  101,  1045,  8823,  ...,     0,     0,     0],\n",
            "        [  101,  2672,  2009,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  8855,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2326,  1010,  ...,     0,     0,     0],\n",
            "        [  101,  2026, 10861,  ...,     0,     0,     0],\n",
            "        [  101,  2027,  2024,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 0, 0, 2, 0, 0], device='cuda:0')\n",
            "logits tensor([[ 2.2037, -2.2378, -0.3879],\n",
            "        [-0.4491, -1.7465,  1.9849],\n",
            "        [-2.6408, -1.9356,  4.4295],\n",
            "        [-2.4733, -1.0599,  3.5011],\n",
            "        [-2.5790, -1.9633,  4.4144],\n",
            "        [-0.1418, -0.5616,  0.4932],\n",
            "        [-2.6069, -1.6329,  4.2198],\n",
            "        [-2.6083, -1.9958,  4.4477],\n",
            "        [ 0.9454, -2.0030,  0.6314],\n",
            "        [ 3.1553, -1.7762, -1.5725],\n",
            "        [-1.9898, -1.8263,  3.6576],\n",
            "        [ 2.8496, -2.2082, -1.0480],\n",
            "        [ 0.2637, -0.7714,  0.4389],\n",
            "        [-2.4495, -2.0469,  4.4351],\n",
            "        [ 3.6873, -2.1463, -1.7458],\n",
            "        [ 4.2525, -1.8616, -2.5791]], device='cuda:0')\n",
            "logits [[ 2.2036638  -2.2377958  -0.38789463]\n",
            " [-0.44908282 -1.7465225   1.9848531 ]\n",
            " [-2.6408067  -1.9355625   4.4295287 ]\n",
            " [-2.4733305  -1.0598524   3.5011084 ]\n",
            " [-2.5789938  -1.9633449   4.4143972 ]\n",
            " [-0.14176609 -0.5615848   0.4931919 ]\n",
            " [-2.6069179  -1.6329256   4.2198176 ]\n",
            " [-2.6083496  -1.9958491   4.447698  ]\n",
            " [ 0.9454269  -2.0029676   0.6313719 ]\n",
            " [ 3.1553407  -1.7761633  -1.5724545 ]\n",
            " [-1.9898221  -1.8262919   3.657564  ]\n",
            " [ 2.8496075  -2.208219   -1.048031  ]\n",
            " [ 0.2637171  -0.77135366  0.4389227 ]\n",
            " [-2.4495285  -2.0469396   4.4351473 ]\n",
            " [ 3.687327   -2.1462538  -1.7458161 ]\n",
            " [ 4.252499   -1.8616287  -2.5791483 ]]\n",
            "label_ids [0 2 2 2 2 2 2 2 0 1 0 0 0 2 0 0]\n",
            "b_input_ids tensor([[  101,  6293,  2007,  ...,     0,     0,     0],\n",
            "        [  101, 25545,  2003,  ...,     0,     0,     0],\n",
            "        [  101,  2307,  5379,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2065,  2017,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  3095,  ...,     0,     0,     0],\n",
            "        [  101,  7297,  1010,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[-2.6505, -1.7817,  4.2673],\n",
            "        [-2.2972, -0.8473,  3.0654],\n",
            "        [-2.7783, -1.7701,  4.4374],\n",
            "        [-2.5341, -2.0371,  4.3711],\n",
            "        [ 1.8253, -1.6170, -0.4073],\n",
            "        [-2.4546, -2.1343,  4.3749],\n",
            "        [ 3.3845, -0.8597, -2.6141],\n",
            "        [-2.4787, -2.1249,  4.4107],\n",
            "        [ 0.7204, -2.0043,  0.9882],\n",
            "        [-2.5741, -1.9940,  4.3844],\n",
            "        [ 1.3852, -2.4002,  0.4649],\n",
            "        [-2.3790, -1.6626,  3.9392],\n",
            "        [-2.5473, -1.9950,  4.3680],\n",
            "        [ 3.8936, -1.3700, -2.5489],\n",
            "        [-2.5686, -2.0115,  4.4519],\n",
            "        [ 4.2221, -2.0814, -2.3524]], device='cuda:0')\n",
            "logits [[-2.6505158  -1.7816831   4.2673097 ]\n",
            " [-2.297193   -0.8473094   3.06543   ]\n",
            " [-2.7783184  -1.7700578   4.437376  ]\n",
            " [-2.5341315  -2.0371096   4.371079  ]\n",
            " [ 1.8253348  -1.6170278  -0.40730625]\n",
            " [-2.4545567  -2.1343045   4.374852  ]\n",
            " [ 3.3845105  -0.8596779  -2.6141126 ]\n",
            " [-2.478661   -2.1248882   4.4107165 ]\n",
            " [ 0.7203841  -2.0043368   0.98815435]\n",
            " [-2.5740528  -1.9940089   4.3843846 ]\n",
            " [ 1.3852198  -2.4001646   0.46489504]\n",
            " [-2.3789787  -1.6625538   3.9392354 ]\n",
            " [-2.5473146  -1.9950187   4.368024  ]\n",
            " [ 3.8935647  -1.369965   -2.548855  ]\n",
            " [-2.5685601  -2.0114765   4.4519157 ]\n",
            " [ 4.2220516  -2.0814443  -2.3524392 ]]\n",
            "label_ids [2 0 2 2 0 2 0 2 0 2 2 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  2079,  3489,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2018,  ...,     0,     0,     0],\n",
            "        [  101,  2293, 10733,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  8840,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2031,  ...,     0,     0,     0],\n",
            "        [  101,  2026,  2684,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[-2.4212, -2.1583,  4.3632],\n",
            "        [-2.4623, -1.8643,  4.2482],\n",
            "        [-2.5714, -1.8425,  4.3525],\n",
            "        [-0.6588,  0.2924,  0.5149],\n",
            "        [ 2.8449, -1.1300, -1.6141],\n",
            "        [ 4.1796, -2.0924, -2.2527],\n",
            "        [-2.5872, -1.9935,  4.3639],\n",
            "        [ 2.9968, -2.3821, -1.0227],\n",
            "        [ 2.2174, -1.9651, -0.5319],\n",
            "        [-2.5685, -1.8527,  4.4125],\n",
            "        [-2.1898, -1.7536,  3.8179],\n",
            "        [ 3.6453, -1.8206, -1.9567],\n",
            "        [ 4.1924, -2.0371, -2.3220],\n",
            "        [-2.5753, -1.5122,  3.8738],\n",
            "        [-1.0245, -2.1103,  2.9878],\n",
            "        [-2.2572, -2.2717,  4.3464]], device='cuda:0')\n",
            "logits [[-2.4211798  -2.1583433   4.3632064 ]\n",
            " [-2.4622974  -1.8642879   4.248177  ]\n",
            " [-2.5713892  -1.8424625   4.352509  ]\n",
            " [-0.6588128   0.29235974  0.51490456]\n",
            " [ 2.8449497  -1.13       -1.6141216 ]\n",
            " [ 4.179612   -2.0924058  -2.2527022 ]\n",
            " [-2.5872493  -1.9935243   4.3639073 ]\n",
            " [ 2.996829   -2.3820949  -1.0226978 ]\n",
            " [ 2.2174137  -1.9650744  -0.53191954]\n",
            " [-2.5684912  -1.8526993   4.412538  ]\n",
            " [-2.1897602  -1.7536315   3.8178546 ]\n",
            " [ 3.645306   -1.8205527  -1.9567243 ]\n",
            " [ 4.1924324  -2.0370607  -2.3219776 ]\n",
            " [-2.575295   -1.5122473   3.8738275 ]\n",
            " [-1.024457   -2.11028     2.987788  ]\n",
            " [-2.257233   -2.2717378   4.346412  ]]\n",
            "label_ids [2 2 2 0 0 0 2 0 0 2 2 0 0 2 2 2]\n",
            "b_input_ids tensor([[ 101, 1996, 2833,  ...,    0,    0,    0],\n",
            "        [ 101, 2057, 2071,  ...,    0,    0,    0],\n",
            "        [ 101, 2023, 2173,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 4468, 2023,  ...,    0,    0,    0],\n",
            "        [ 101, 2673, 2001,  ...,    0,    0,    0],\n",
            "        [ 101, 2205, 2919,  ...,    0,    0,    0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[-2.3615, -2.2124,  4.3727],\n",
            "        [-1.7934, -1.0341,  2.7715],\n",
            "        [ 0.3079, -1.0035,  0.6308],\n",
            "        [-2.4906, -2.1131,  4.4374],\n",
            "        [-2.5339, -1.8514,  4.3880],\n",
            "        [-2.4090, -2.1102,  4.3424],\n",
            "        [-0.2543, -1.2667,  1.2835],\n",
            "        [-2.3921, -2.1889,  4.4002],\n",
            "        [-2.5519, -2.0048,  4.4501],\n",
            "        [-2.5645, -1.9763,  4.3883],\n",
            "        [-2.4997, -1.8674,  4.3391],\n",
            "        [-2.5872, -2.0219,  4.4435],\n",
            "        [-2.6483, -1.8826,  4.4372],\n",
            "        [ 3.0692, -1.7321, -1.6382],\n",
            "        [-2.5284, -2.0195,  4.4057],\n",
            "        [ 3.4720, -1.5974, -1.9250]], device='cuda:0')\n",
            "logits [[-2.361476   -2.2123952   4.3726983 ]\n",
            " [-1.7933784  -1.0341463   2.7715204 ]\n",
            " [ 0.30787015 -1.003485    0.6308083 ]\n",
            " [-2.490633   -2.1131284   4.4374304 ]\n",
            " [-2.5339108  -1.8514153   4.388045  ]\n",
            " [-2.4090261  -2.1102033   4.3423553 ]\n",
            " [-0.25428155 -1.2667155   1.2835457 ]\n",
            " [-2.3920696  -2.1889267   4.400225  ]\n",
            " [-2.5519152  -2.004827    4.4500556 ]\n",
            " [-2.5645335  -1.9763236   4.3883224 ]\n",
            " [-2.4997163  -1.867354    4.3390594 ]\n",
            " [-2.5872266  -2.0218878   4.443523  ]\n",
            " [-2.6483343  -1.882573    4.43722   ]\n",
            " [ 3.0691574  -1.7320604  -1.6381885 ]\n",
            " [-2.528436   -2.019507    4.4057207 ]\n",
            " [ 3.4719794  -1.5973622  -1.9250187 ]]\n",
            "label_ids [2 2 0 2 2 2 2 2 2 2 2 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  1996,  2833,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2001,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2326,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  2131,  1996,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 17917,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[-2.4951, -1.4289,  3.8527],\n",
            "        [-1.4799, -2.2096,  3.4448],\n",
            "        [ 4.2738, -1.9301, -2.5999],\n",
            "        [-2.7237, -1.8246,  4.4409],\n",
            "        [-2.1601, -2.2664,  4.1816],\n",
            "        [-0.8972, -1.6186,  2.2518],\n",
            "        [ 1.1471, -1.2555, -0.1035],\n",
            "        [-2.5610, -1.6873,  4.2372],\n",
            "        [-2.6897, -1.8773,  4.4267],\n",
            "        [-0.9151, -0.0639,  1.2235],\n",
            "        [-0.4494,  3.3380, -2.1790],\n",
            "        [-1.3541, -0.6852,  1.9870],\n",
            "        [ 2.1021, -1.6668, -0.7736],\n",
            "        [-2.5835, -1.9681,  4.3974],\n",
            "        [-2.6910, -1.8294,  4.3890],\n",
            "        [ 3.0256, -2.0879, -1.2015]], device='cuda:0')\n",
            "logits [[-2.495088   -1.4288715   3.8526747 ]\n",
            " [-1.479929   -2.2095926   3.4447634 ]\n",
            " [ 4.2738414  -1.9301007  -2.5998511 ]\n",
            " [-2.7236915  -1.8245829   4.440897  ]\n",
            " [-2.160143   -2.2663548   4.181564  ]\n",
            " [-0.8972477  -1.6185541   2.2517617 ]\n",
            " [ 1.1470927  -1.2555496  -0.10346261]\n",
            " [-2.561005   -1.687338    4.237243  ]\n",
            " [-2.689729   -1.8773127   4.4267397 ]\n",
            " [-0.91507524 -0.06391237  1.2235494 ]\n",
            " [-0.44935116  3.3379853  -2.1790383 ]\n",
            " [-1.354109   -0.68517077  1.9870354 ]\n",
            " [ 2.10209    -1.6668249  -0.77357656]\n",
            " [-2.583531   -1.9681431   4.3973637 ]\n",
            " [-2.6910233  -1.8293576   4.389004  ]\n",
            " [ 3.025557   -2.0879424  -1.2015322 ]]\n",
            "label_ids [2 2 0 2 2 2 0 1 2 0 0 2 2 2 2 2]\n",
            "b_input_ids tensor([[ 101, 2065, 2017,  ...,    0,    0,    0],\n",
            "        [ 101, 2000, 2022,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 8840,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2204, 2833,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 2686,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 2326,  ...,    0,    0,    0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[ 2.9201, -2.3300, -0.9755],\n",
            "        [-0.0720, -1.8807,  1.6476],\n",
            "        [-2.5494, -2.0219,  4.3349],\n",
            "        [-0.4779, -1.7802,  1.7842],\n",
            "        [-2.3403, -1.6802,  3.9109],\n",
            "        [ 0.4028, -0.5926,  0.2232],\n",
            "        [-2.6114, -1.9318,  4.4513],\n",
            "        [ 2.4821, -2.2347, -0.7468],\n",
            "        [ 3.6052, -1.9493, -1.8519],\n",
            "        [-2.4044, -1.3394,  3.6066],\n",
            "        [ 3.8788, -2.1399, -2.0139],\n",
            "        [ 0.2644, -0.1475,  0.1017],\n",
            "        [-2.6618, -1.8407,  4.4492],\n",
            "        [-2.6900, -1.2117,  3.8217],\n",
            "        [ 0.9646, -1.6989,  0.5643],\n",
            "        [-2.5561, -2.0212,  4.4412]], device='cuda:0')\n",
            "logits [[ 2.9200656  -2.32997    -0.9755347 ]\n",
            " [-0.0720019  -1.8806821   1.6476055 ]\n",
            " [-2.5493655  -2.0219297   4.3348894 ]\n",
            " [-0.47786286 -1.7802035   1.7841868 ]\n",
            " [-2.3403034  -1.68018     3.9108808 ]\n",
            " [ 0.40284094 -0.5926008   0.22324684]\n",
            " [-2.6113973  -1.9318244   4.4513016 ]\n",
            " [ 2.4820828  -2.2347379  -0.74684155]\n",
            " [ 3.6052012  -1.9493349  -1.8519207 ]\n",
            " [-2.4044132  -1.3393945   3.6066425 ]\n",
            " [ 3.8787858  -2.1398685  -2.0139065 ]\n",
            " [ 0.2643774  -0.14754418  0.10170951]\n",
            " [-2.6617923  -1.8406854   4.4492083 ]\n",
            " [-2.6899557  -1.2116512   3.8216844 ]\n",
            " [ 0.96456087 -1.6988789   0.5642644 ]\n",
            " [-2.5561182  -2.021184    4.4412346 ]]\n",
            "label_ids [0 0 2 2 2 2 2 0 0 2 0 2 2 2 2 2]\n",
            "b_input_ids tensor([[  101,  1996,  2173,  2003,  1037, 20377, 13181,  2029,  2965,  1024,\n",
            "          3722, 10447,  1998,  4511,  2366, 18228,  1999,  1037, 13950,  2989,\n",
            "          7224,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  3071, 23289,  2094,  2055,  1996,  7224,  1006, 11552,  4734,\n",
            "          1998,  7078,  4297, 25377, 25236,  5328,  1007,  1998,  1996, 18783,\n",
            "          2833,   999,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1996,  2833,  2001, 19960,  3695, 16748,  2012,  2190,  2021,\n",
            "          2009,  2001,  1996,  9202,  2326,  2008,  2081,  2033, 19076,  2196,\n",
            "          2000,  2175,  2067,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2023, 24209, 22325,  1998,  6298, 19817, 19321, 11069,  2003,\n",
            "          2012,  1996,  2327,  1997,  2026,  7128,  4825,  2862,  1012,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2031,  1996, 28248,  5572,  1012,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2307,  2833,  1010,  2307, 25545,  1010,  2307,  2326,  1012,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1045,  2031,  2042,  2000, 12211,  1005,  1055,  3807,  1998,\n",
            "          2119,  2335,  2020,  2200, 15640,  1012,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "       device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  25%|██▌       | 1/4 [00:24<01:14, 24.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 0], device='cuda:0')\n",
            "logits tensor([[-2.6262, -1.9802,  4.4526],\n",
            "        [-2.5127, -2.0612,  4.4530],\n",
            "        [ 2.9920, -2.1023, -1.1829],\n",
            "        [-2.4463, -2.1425,  4.4027],\n",
            "        [-2.7274, -1.1422,  3.9201],\n",
            "        [-2.7237, -1.8246,  4.4409],\n",
            "        [ 1.1031, -2.0299,  0.6033]], device='cuda:0')\n",
            "logits [[-2.626217   -1.9802266   4.4525523 ]\n",
            " [-2.51273    -2.061172    4.453023  ]\n",
            " [ 2.9919639  -2.102328   -1.1828957 ]\n",
            " [-2.446289   -2.1425111   4.40272   ]\n",
            " [-2.7274108  -1.1422318   3.9201176 ]\n",
            " [-2.7236915  -1.8245829   4.440897  ]\n",
            " [ 1.103122   -2.0299344   0.60330844]]\n",
            "label_ids [2 2 0 2 2 2 0]\n",
            "Validation Accuracy: 0.85625\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.4544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Train loss: 0.11657958269557532\n",
            "b_input_ids tensor([[  101,  1996, 13877,  ...,     0,     0,     0],\n",
            "        [  101,  2061,  2054,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2326,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2123,  1005,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2069,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0], device='cuda:0')\n",
            "logits tensor([[ 3.6996, -2.5405, -1.5526],\n",
            "        [ 3.7894, -1.8087, -2.0428],\n",
            "        [-2.9812, -1.6167,  4.3082],\n",
            "        [-3.0573, -1.3108,  4.1503],\n",
            "        [-3.0881, -1.7622,  4.6118],\n",
            "        [-2.3262, -1.4595,  3.3691],\n",
            "        [-2.7024, -2.0936,  4.5593],\n",
            "        [ 4.4401, -2.1531, -2.4851],\n",
            "        [-2.3000, -0.5182,  2.6529],\n",
            "        [-3.0677, -1.0450,  3.8694],\n",
            "        [-3.1435, -1.8178,  4.6344],\n",
            "        [ 1.1554,  0.4543, -1.2243],\n",
            "        [-1.3280, -1.6250,  2.7662],\n",
            "        [ 2.6404, -1.9428, -0.8702],\n",
            "        [-1.5019, -2.4573,  3.4632],\n",
            "        [ 0.8439, -0.5609, -0.0921]], device='cuda:0')\n",
            "logits [[ 3.6995995  -2.54045    -1.5525503 ]\n",
            " [ 3.7893524  -1.8087459  -2.0427814 ]\n",
            " [-2.9812493  -1.6166836   4.3081527 ]\n",
            " [-3.0572693  -1.3108033   4.1502576 ]\n",
            " [-3.0880644  -1.7622488   4.6118    ]\n",
            " [-2.3261588  -1.4595462   3.369111  ]\n",
            " [-2.7023683  -2.0936294   4.559269  ]\n",
            " [ 4.4401298  -2.1531184  -2.4851131 ]\n",
            " [-2.2999694  -0.5181539   2.6529255 ]\n",
            " [-3.0677326  -1.045007    3.869446  ]\n",
            " [-3.1434724  -1.8177757   4.6344194 ]\n",
            " [ 1.1553642   0.45430055 -1.2243353 ]\n",
            " [-1.328046   -1.6249831   2.7662401 ]\n",
            " [ 2.6403768  -1.9427692  -0.8702279 ]\n",
            " [-1.5018848  -2.4573274   3.463194  ]\n",
            " [ 0.843922   -0.56093705 -0.09209556]]\n",
            "label_ids [0 1 2 2 2 2 2 0 2 2 2 2 2 2 2 0]\n",
            "b_input_ids tensor([[  101, 13558, 18064,  ...,     0,     0,     0],\n",
            "        [  101,  1037,  2367,  ...,     0,     0,     0],\n",
            "        [  101,  2296,  2051,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  2173,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2428,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 18499,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 0, 2, 1, 2, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[ 3.6315, -1.3946, -2.1481],\n",
            "        [-1.4184, -1.6344,  2.5969],\n",
            "        [-2.8196, -1.9867,  4.5355],\n",
            "        [-1.4036,  2.7556, -0.4955],\n",
            "        [-2.9276, -0.9027,  3.6494],\n",
            "        [-1.6195, -1.4775,  2.7207],\n",
            "        [-0.9068,  3.5857, -1.7361],\n",
            "        [ 3.7842, -2.2333, -1.7525],\n",
            "        [-3.1618, -1.6098,  4.4924],\n",
            "        [ 1.0608, -1.5248,  0.3343],\n",
            "        [ 3.9917, -2.8258, -1.6772],\n",
            "        [-3.0925, -1.6511,  4.4436],\n",
            "        [-2.8457, -0.8747,  3.4151],\n",
            "        [ 0.2267, -1.4935,  1.1173],\n",
            "        [-3.2062, -1.7042,  4.6341],\n",
            "        [ 3.9071, -2.7514, -1.6630]], device='cuda:0')\n",
            "logits [[ 3.6314719  -1.3945915  -2.1481388 ]\n",
            " [-1.4184105  -1.6343734   2.5968604 ]\n",
            " [-2.8195765  -1.9866985   4.535494  ]\n",
            " [-1.4035819   2.7556295  -0.49547404]\n",
            " [-2.9276018  -0.9026916   3.6494014 ]\n",
            " [-1.6194786  -1.4774773   2.7206585 ]\n",
            " [-0.906832    3.5857022  -1.7361246 ]\n",
            " [ 3.7842367  -2.233337   -1.7525414 ]\n",
            " [-3.1618245  -1.6098202   4.4924088 ]\n",
            " [ 1.0608236  -1.5248126   0.33429182]\n",
            " [ 3.9917102  -2.8257751  -1.6771852 ]\n",
            " [-3.09248    -1.6510592   4.4435687 ]\n",
            " [-2.8457181  -0.87473583  3.415052  ]\n",
            " [ 0.22671835 -1.4935337   1.1172527 ]\n",
            " [-3.2062056  -1.704246    4.634136  ]\n",
            " [ 3.9070594  -2.7514071  -1.6630026 ]]\n",
            "label_ids [0 0 2 1 2 2 2 0 2 0 0 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  1045,  3811,  ...,     0,     0,     0],\n",
            "        [  101,  2025,  2069,  ...,     0,     0,     0],\n",
            "        [  101,  2017,  2442,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  4664,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 25545,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2173,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2], device='cuda:0')\n",
            "logits tensor([[-3.0745, -1.9254,  4.6642],\n",
            "        [-3.0341, -1.7374,  4.4626],\n",
            "        [-2.8145, -1.9446,  4.4161],\n",
            "        [-2.5339, -0.8313,  3.1679],\n",
            "        [-3.0095, -1.8775,  4.5727],\n",
            "        [ 4.1101, -2.7322, -1.7636],\n",
            "        [-2.4998, -0.4010,  2.8311],\n",
            "        [ 0.5425, -1.2979,  0.7343],\n",
            "        [-2.2410, -1.2841,  3.0928],\n",
            "        [-2.5439, -1.5727,  3.5969],\n",
            "        [-3.0677, -1.0450,  3.8694],\n",
            "        [-2.9981, -1.0454,  3.8549],\n",
            "        [ 4.5693, -2.4996, -2.3452],\n",
            "        [-3.0775, -1.0240,  3.8997],\n",
            "        [ 2.3132, -1.6512, -0.8983],\n",
            "        [-2.8099, -2.1004,  4.6513]], device='cuda:0')\n",
            "logits [[-3.0745037  -1.9254147   4.664158  ]\n",
            " [-3.0340765  -1.7374105   4.462615  ]\n",
            " [-2.8144867  -1.9445587   4.4161315 ]\n",
            " [-2.533881   -0.83127195  3.1678915 ]\n",
            " [-3.0095289  -1.8775172   4.572715  ]\n",
            " [ 4.110051   -2.732182   -1.7635998 ]\n",
            " [-2.4997554  -0.40103686  2.831082  ]\n",
            " [ 0.54246306 -1.2979014   0.73429835]\n",
            " [-2.2410262  -1.284067    3.0927815 ]\n",
            " [-2.543862   -1.57271     3.5969472 ]\n",
            " [-3.0677326  -1.045007    3.869446  ]\n",
            " [-2.9981043  -1.0454159   3.8549495 ]\n",
            " [ 4.5693154  -2.4995737  -2.3451796 ]\n",
            " [-3.0774543  -1.0240245   3.8997278 ]\n",
            " [ 2.3132172  -1.6512148  -0.89828604]\n",
            " [-2.8099036  -2.1004333   4.6512847 ]]\n",
            "label_ids [2 2 2 2 2 0 2 0 2 2 2 2 0 2 0 2]\n",
            "b_input_ids tensor([[  101,  1045,  8823,  ...,     0,     0,     0],\n",
            "        [  101,  2672,  2009,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  8855,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2326,  1010,  ...,     0,     0,     0],\n",
            "        [  101,  2026, 10861,  ...,     0,     0,     0],\n",
            "        [  101,  2027,  2024,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 0, 0, 2, 0, 0], device='cuda:0')\n",
            "logits tensor([[ 3.7363, -2.8398, -1.4331],\n",
            "        [-1.3861, -1.5346,  2.5590],\n",
            "        [-3.1508, -1.3799,  4.2899],\n",
            "        [-2.7106, -0.8583,  3.3895],\n",
            "        [-3.0504, -1.7633,  4.5685],\n",
            "        [ 0.5660, -0.6480, -0.0968],\n",
            "        [-2.3866, -1.0504,  3.1240],\n",
            "        [-3.0208, -1.5318,  4.2465],\n",
            "        [ 1.1186, -2.1996,  0.5781],\n",
            "        [ 4.0829, -2.7561, -1.7872],\n",
            "        [-1.7236, -1.6296,  2.9247],\n",
            "        [ 3.8195, -2.8277, -1.5622],\n",
            "        [-0.0251, -0.8675,  0.7623],\n",
            "        [-2.8467, -1.7494,  4.3605],\n",
            "        [ 4.4138, -2.5491, -2.0951],\n",
            "        [ 4.6943, -2.4012, -2.4944]], device='cuda:0')\n",
            "logits [[ 3.7362592  -2.8397903  -1.4330525 ]\n",
            " [-1.3861284  -1.5345596   2.559013  ]\n",
            " [-3.1508472  -1.379878    4.2899    ]\n",
            " [-2.7105846  -0.8583028   3.3894916 ]\n",
            " [-3.050387   -1.7632834   4.568466  ]\n",
            " [ 0.5659844  -0.64803904 -0.09675843]\n",
            " [-2.3865578  -1.0503945   3.124034  ]\n",
            " [-3.0207744  -1.5317731   4.246519  ]\n",
            " [ 1.1185894  -2.1995783   0.5781046 ]\n",
            " [ 4.0828824  -2.756103   -1.7871739 ]\n",
            " [-1.7236193  -1.6295553   2.9246924 ]\n",
            " [ 3.8195362  -2.8277278  -1.5621947 ]\n",
            " [-0.02510013 -0.8675347   0.7623393 ]\n",
            " [-2.84668    -1.7494109   4.360461  ]\n",
            " [ 4.413817   -2.549059   -2.0950828 ]\n",
            " [ 4.694264   -2.4012368  -2.494352  ]]\n",
            "label_ids [0 2 2 2 2 2 2 2 0 1 0 0 0 2 0 0]\n",
            "b_input_ids tensor([[  101,  6293,  2007,  ...,     0,     0,     0],\n",
            "        [  101, 25545,  2003,  ...,     0,     0,     0],\n",
            "        [  101,  2307,  5379,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2065,  2017,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  3095,  ...,     0,     0,     0],\n",
            "        [  101,  7297,  1010,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[-3.0925, -1.6511,  4.4436],\n",
            "        [-1.6008, -0.1773,  1.7845],\n",
            "        [-2.9607, -0.8708,  3.6208],\n",
            "        [-3.1435, -1.8178,  4.6344],\n",
            "        [ 3.8128, -2.0315, -1.9221],\n",
            "        [-3.0788, -1.9498,  4.6852],\n",
            "        [ 4.5467, -1.9062, -2.7828],\n",
            "        [-3.0745, -1.9254,  4.6642],\n",
            "        [ 1.1453, -2.3679,  0.7895],\n",
            "        [-3.1301, -1.8402,  4.6605],\n",
            "        [ 2.5646, -3.0348, -0.3481],\n",
            "        [-1.5995, -1.4166,  2.6184],\n",
            "        [-3.0467, -1.5641,  4.2812],\n",
            "        [ 4.5433, -2.3386, -2.4401],\n",
            "        [-3.0539, -1.6426,  4.4257],\n",
            "        [ 4.4479, -2.6713, -2.1178]], device='cuda:0')\n",
            "logits [[-3.09248    -1.6510592   4.4435687 ]\n",
            " [-1.6007966  -0.17734423  1.7844995 ]\n",
            " [-2.9607232  -0.87079704  3.6207588 ]\n",
            " [-3.1434724  -1.8177757   4.6344194 ]\n",
            " [ 3.8128102  -2.0315056  -1.9220662 ]\n",
            " [-3.078807   -1.9497755   4.685157  ]\n",
            " [ 4.5467057  -1.9061968  -2.782825  ]\n",
            " [-3.0745037  -1.9254147   4.664158  ]\n",
            " [ 1.1453148  -2.3678913   0.78953636]\n",
            " [-3.1301177  -1.8402414   4.660452  ]\n",
            " [ 2.5646374  -3.0347738  -0.34814033]\n",
            " [-1.5994567  -1.416556    2.6183805 ]\n",
            " [-3.0466802  -1.5641425   4.281167  ]\n",
            " [ 4.5433397  -2.338594   -2.4400709 ]\n",
            " [-3.0538976  -1.6426234   4.425703  ]\n",
            " [ 4.4478936  -2.6712525  -2.1178281 ]]\n",
            "label_ids [2 0 2 2 0 2 0 2 0 2 2 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  2079,  3489,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2018,  ...,     0,     0,     0],\n",
            "        [  101,  2293, 10733,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  8840,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2031,  ...,     0,     0,     0],\n",
            "        [  101,  2026,  2684,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[-2.9710, -2.0005,  4.6041],\n",
            "        [-2.5735, -1.6030,  3.8193],\n",
            "        [-3.0418, -1.5899,  4.4150],\n",
            "        [-0.7703,  0.0389,  0.8646],\n",
            "        [ 4.7480, -1.8481, -3.0699],\n",
            "        [ 4.3789, -2.5937, -2.0660],\n",
            "        [-3.2080, -1.6902,  4.5785],\n",
            "        [ 4.1987, -2.8259, -1.8583],\n",
            "        [ 3.7064, -2.4384, -1.6083],\n",
            "        [-2.7506, -1.3426,  3.9024],\n",
            "        [-1.2875, -1.3301,  2.2935],\n",
            "        [ 4.2967, -2.6375, -2.0357],\n",
            "        [ 4.2964, -2.6392, -2.0241],\n",
            "        [-3.0281, -1.8646,  4.5273],\n",
            "        [-1.0673, -2.2598,  2.9982],\n",
            "        [-2.7094, -2.2687,  4.6424]], device='cuda:0')\n",
            "logits [[-2.971029   -2.0005286   4.604126  ]\n",
            " [-2.5735378  -1.6029992   3.8192873 ]\n",
            " [-3.0418417  -1.5898738   4.415007  ]\n",
            " [-0.77031374  0.03892455  0.86461765]\n",
            " [ 4.7479525  -1.848099   -3.069897  ]\n",
            " [ 4.378892   -2.5937047  -2.0659673 ]\n",
            " [-3.2079923  -1.6902328   4.578519  ]\n",
            " [ 4.1986747  -2.8259478  -1.8582522 ]\n",
            " [ 3.7063687  -2.4383543  -1.6083183 ]\n",
            " [-2.7506363  -1.3425586   3.902441  ]\n",
            " [-1.2875459  -1.3301427   2.2934585 ]\n",
            " [ 4.296655   -2.6374936  -2.0357323 ]\n",
            " [ 4.2963643  -2.639245   -2.0240874 ]\n",
            " [-3.028127   -1.8645608   4.527334  ]\n",
            " [-1.0673     -2.2597878   2.998196  ]\n",
            " [-2.7094147  -2.268674    4.6423707 ]]\n",
            "label_ids [2 2 2 0 0 0 2 0 0 2 2 0 0 2 2 2]\n",
            "b_input_ids tensor([[ 101, 1996, 2833,  ...,    0,    0,    0],\n",
            "        [ 101, 2057, 2071,  ...,    0,    0,    0],\n",
            "        [ 101, 2023, 2173,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 4468, 2023,  ...,    0,    0,    0],\n",
            "        [ 101, 2673, 2001,  ...,    0,    0,    0],\n",
            "        [ 101, 2205, 2919,  ...,    0,    0,    0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[-2.8306, -2.1425,  4.5861],\n",
            "        [-1.9552, -0.8819,  2.6912],\n",
            "        [ 0.0999, -1.0683,  0.9277],\n",
            "        [-2.8524, -1.8727,  4.3652],\n",
            "        [-2.7409, -1.3358,  3.8398],\n",
            "        [-2.8103, -1.8821,  4.2643],\n",
            "        [-0.1342, -1.2524,  1.1316],\n",
            "        [-2.9311, -1.9812,  4.5932],\n",
            "        [-2.9313, -1.6787,  4.3194],\n",
            "        [-3.2107, -1.6309,  4.6063],\n",
            "        [-2.7591, -1.3461,  3.8417],\n",
            "        [-3.0540, -1.6432,  4.3762],\n",
            "        [-3.0931, -1.3794,  4.2512],\n",
            "        [ 3.6220, -2.6872, -1.5942],\n",
            "        [-3.1217, -1.7287,  4.5772],\n",
            "        [ 4.4574, -1.9879, -2.5834]], device='cuda:0')\n",
            "logits [[-2.8306017  -2.1424863   4.5860677 ]\n",
            " [-1.9552053  -0.88189083  2.6911685 ]\n",
            " [ 0.09986548 -1.0683028   0.9276823 ]\n",
            " [-2.852365   -1.8726926   4.365182  ]\n",
            " [-2.7408752  -1.3357937   3.8398087 ]\n",
            " [-2.810342   -1.8821353   4.2642765 ]\n",
            " [-0.13422425 -1.2524341   1.1315973 ]\n",
            " [-2.9310868  -1.9812158   4.593209  ]\n",
            " [-2.9312644  -1.678653    4.31936   ]\n",
            " [-3.2107275  -1.630863    4.60626   ]\n",
            " [-2.7590773  -1.3460941   3.841731  ]\n",
            " [-3.053991   -1.6431825   4.3762054 ]\n",
            " [-3.0931287  -1.3794298   4.251211  ]\n",
            " [ 3.6220143  -2.6871712  -1.5941852 ]\n",
            " [-3.121749   -1.7287441   4.577159  ]\n",
            " [ 4.4573503  -1.9878863  -2.5834117 ]]\n",
            "label_ids [2 2 0 2 2 2 2 2 2 2 2 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  1996,  2833,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2001,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2326,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  2131,  1996,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 17917,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[-2.8159, -1.3849,  3.8711],\n",
            "        [-1.5019, -2.4573,  3.4632],\n",
            "        [ 4.5177, -2.5658, -2.2014],\n",
            "        [-3.1033, -1.1322,  4.0235],\n",
            "        [-2.4932, -2.3645,  4.4104],\n",
            "        [-1.5235, -2.0442,  3.0535],\n",
            "        [ 0.9540, -1.4608,  0.2825],\n",
            "        [-2.6124, -1.0659,  3.3723],\n",
            "        [-2.9922, -1.6356,  4.2803],\n",
            "        [-0.2900, -0.0944,  0.6045],\n",
            "        [-0.9236,  3.5005, -1.7578],\n",
            "        [-1.9621, -0.6982,  2.4472],\n",
            "        [ 1.6719, -1.7892, -0.2748],\n",
            "        [-3.1298, -1.3973,  4.2757],\n",
            "        [-3.1775, -1.3613,  4.2853],\n",
            "        [ 4.2269, -2.6690, -1.9713]], device='cuda:0')\n",
            "logits [[-2.8159344  -1.3848721   3.8710856 ]\n",
            " [-1.5018848  -2.4573274   3.463194  ]\n",
            " [ 4.5176764  -2.5658321  -2.2014203 ]\n",
            " [-3.1033201  -1.1322436   4.023468  ]\n",
            " [-2.493194   -2.3645487   4.410428  ]\n",
            " [-1.5234867  -2.044175    3.0535374 ]\n",
            " [ 0.9540164  -1.4608271   0.28248692]\n",
            " [-2.6123784  -1.065883    3.3722978 ]\n",
            " [-2.9922116  -1.6355865   4.2802672 ]\n",
            " [-0.29003343 -0.09438564  0.6045255 ]\n",
            " [-0.92364144  3.5004945  -1.7577931 ]\n",
            " [-1.9621056  -0.69822806  2.44719   ]\n",
            " [ 1.6718992  -1.7892315  -0.2747898 ]\n",
            " [-3.1297724  -1.3973129   4.275653  ]\n",
            " [-3.17753    -1.3612794   4.2852764 ]\n",
            " [ 4.226878   -2.6689649  -1.9713237 ]]\n",
            "label_ids [2 2 0 2 2 2 0 1 2 0 0 2 2 2 2 2]\n",
            "b_input_ids tensor([[ 101, 2065, 2017,  ...,    0,    0,    0],\n",
            "        [ 101, 2000, 2022,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 8840,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2204, 2833,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 2686,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 2326,  ...,    0,    0,    0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[ 4.1103, -2.8888, -1.7612],\n",
            "        [-1.5539, -2.0586,  3.0008],\n",
            "        [-2.9297, -2.0204,  4.5438],\n",
            "        [-0.1579, -2.1279,  1.6634],\n",
            "        [-2.6097, -1.2296,  3.4456],\n",
            "        [ 1.2701, -0.7340, -0.4453],\n",
            "        [-3.1469, -1.5260,  4.4367],\n",
            "        [ 3.5158, -2.7753, -1.4141],\n",
            "        [ 4.3457, -2.5978, -2.1423],\n",
            "        [-2.2460, -1.0711,  3.0096],\n",
            "        [ 4.3463, -2.6978, -2.0960],\n",
            "        [-0.4778, -0.3713,  0.9916],\n",
            "        [-2.9010, -1.2296,  3.8314],\n",
            "        [-1.9714,  0.1468,  1.8531],\n",
            "        [ 1.9373, -2.3422, -0.0242],\n",
            "        [-3.1018, -1.5472,  4.3873]], device='cuda:0')\n",
            "logits [[ 4.110349   -2.8888083  -1.7611817 ]\n",
            " [-1.5539448  -2.0586245   3.0007856 ]\n",
            " [-2.929692   -2.0204241   4.5437956 ]\n",
            " [-0.1578945  -2.1279402   1.6634051 ]\n",
            " [-2.6097472  -1.229605    3.4456396 ]\n",
            " [ 1.2700936  -0.734001   -0.44527334]\n",
            " [-3.1468596  -1.5259653   4.4367247 ]\n",
            " [ 3.515835   -2.775289   -1.4141294 ]\n",
            " [ 4.345733   -2.597798   -2.1422594 ]\n",
            " [-2.2459729  -1.0710934   3.0096078 ]\n",
            " [ 4.34631    -2.6978307  -2.0960245 ]\n",
            " [-0.47782466 -0.37134835  0.99155277]\n",
            " [-2.900968   -1.2295725   3.8314133 ]\n",
            " [-1.9713539   0.14683712  1.8531255 ]\n",
            " [ 1.9372679  -2.342165   -0.02423663]\n",
            " [-3.1018302  -1.5472145   4.3872776 ]]\n",
            "label_ids [0 0 2 2 2 2 2 0 0 2 0 2 2 2 2 2]\n",
            "b_input_ids tensor([[  101,  1996,  2173,  2003,  1037, 20377, 13181,  2029,  2965,  1024,\n",
            "          3722, 10447,  1998,  4511,  2366, 18228,  1999,  1037, 13950,  2989,\n",
            "          7224,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  3071, 23289,  2094,  2055,  1996,  7224,  1006, 11552,  4734,\n",
            "          1998,  7078,  4297, 25377, 25236,  5328,  1007,  1998,  1996, 18783,\n",
            "          2833,   999,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1996,  2833,  2001, 19960,  3695, 16748,  2012,  2190,  2021,\n",
            "          2009,  2001,  1996,  9202,  2326,  2008,  2081,  2033, 19076,  2196,\n",
            "          2000,  2175,  2067,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2023, 24209, 22325,  1998,  6298, 19817, 19321, 11069,  2003,\n",
            "          2012,  1996,  2327,  1997,  2026,  7128,  4825,  2862,  1012,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2031,  1996, 28248,  5572,  1012,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2307,  2833,  1010,  2307, 25545,  1010,  2307,  2326,  1012,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1045,  2031,  2042,  2000, 12211,  1005,  1055,  3807,  1998,\n",
            "          2119,  2335,  2020,  2200, 15640,  1012,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "       device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  50%|█████     | 2/4 [00:49<00:49, 24.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 0], device='cuda:0')\n",
            "logits tensor([[-2.9916, -1.6357,  4.2857],\n",
            "        [-2.9490, -1.8108,  4.4820],\n",
            "        [ 4.1486, -2.8885, -1.7766],\n",
            "        [-3.0204, -1.9635,  4.6607],\n",
            "        [-2.9805, -0.7006,  3.5577],\n",
            "        [-3.1033, -1.1322,  4.0235],\n",
            "        [ 2.0095, -2.8280,  0.1558]], device='cuda:0')\n",
            "logits [[-2.9916427  -1.6357284   4.2856817 ]\n",
            " [-2.9490192  -1.8108325   4.4820385 ]\n",
            " [ 4.148618   -2.8885407  -1.7766497 ]\n",
            " [-3.0204256  -1.9634845   4.660708  ]\n",
            " [-2.9804628  -0.7005878   3.557734  ]\n",
            " [-3.1033201  -1.1322436   4.023468  ]\n",
            " [ 2.0094504  -2.8279686   0.15578772]]\n",
            "label_ids [2 2 0 2 2 2 0]\n",
            "Validation Accuracy: 0.85625\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Train loss: 0.09601442217826843\n",
            "b_input_ids tensor([[  101,  1996, 13877,  ...,     0,     0,     0],\n",
            "        [  101,  2061,  2054,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2326,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2123,  1005,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2069,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0], device='cuda:0')\n",
            "logits tensor([[ 4.5110, -2.5215, -2.3005],\n",
            "        [ 3.8050, -0.9018, -2.9297],\n",
            "        [-2.9533, -2.2970,  4.8538],\n",
            "        [-3.2355, -2.0248,  4.9417],\n",
            "        [-2.9749, -2.3196,  4.8988],\n",
            "        [-2.8445, -2.0871,  4.5860],\n",
            "        [-2.6422, -2.5208,  4.8025],\n",
            "        [ 4.6791, -2.2133, -2.6481],\n",
            "        [-3.2608, -1.2480,  4.3077],\n",
            "        [-3.3120, -1.9771,  4.8899],\n",
            "        [-2.9511, -2.3572,  4.8447],\n",
            "        [ 1.3207,  0.8053, -1.8059],\n",
            "        [-1.5476, -1.8949,  3.2501],\n",
            "        [ 3.2531, -1.5691, -1.6686],\n",
            "        [-2.1397, -2.8199,  4.5350],\n",
            "        [ 0.3538,  0.6642, -0.6304]], device='cuda:0')\n",
            "logits [[ 4.5109715  -2.5214746  -2.30048   ]\n",
            " [ 3.8049905  -0.9017783  -2.9297273 ]\n",
            " [-2.9533036  -2.296978    4.853752  ]\n",
            " [-3.2354772  -2.0247731   4.941678  ]\n",
            " [-2.974852   -2.3196075   4.8987856 ]\n",
            " [-2.8444595  -2.087117    4.5860305 ]\n",
            " [-2.6422377  -2.5208304   4.802548  ]\n",
            " [ 4.679136   -2.213322   -2.6481493 ]\n",
            " [-3.2607768  -1.2480174   4.307699  ]\n",
            " [-3.3119874  -1.9770774   4.88992   ]\n",
            " [-2.9510741  -2.3572078   4.8446817 ]\n",
            " [ 1.3206913   0.8052797  -1.8059158 ]\n",
            " [-1.5476497  -1.894889    3.250123  ]\n",
            " [ 3.2530932  -1.5690799  -1.6686282 ]\n",
            " [-2.1397452  -2.8198786   4.5349774 ]\n",
            " [ 0.35383302  0.664247   -0.6303935 ]]\n",
            "label_ids [0 1 2 2 2 2 2 0 2 2 2 2 2 2 2 0]\n",
            "b_input_ids tensor([[  101, 13558, 18064,  ...,     0,     0,     0],\n",
            "        [  101,  1037,  2367,  ...,     0,     0,     0],\n",
            "        [  101,  2296,  2051,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  2173,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2428,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 18499,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 0, 2, 1, 2, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[ 4.2538, -1.5245, -2.6873],\n",
            "        [-1.8565, -2.0076,  3.3819],\n",
            "        [-2.7608, -2.4410,  4.7943],\n",
            "        [-1.2162,  3.5308, -1.5296],\n",
            "        [-3.3802, -1.7989,  4.8916],\n",
            "        [-0.5959, -1.2970,  1.6335],\n",
            "        [-0.7966,  3.6899, -2.0602],\n",
            "        [ 4.3017, -1.2599, -3.1066],\n",
            "        [-3.1093, -2.2132,  4.8762],\n",
            "        [ 1.1767, -0.8967, -0.2182],\n",
            "        [ 4.4993, -2.5955, -2.2218],\n",
            "        [-2.8592, -2.3517,  4.7580],\n",
            "        [-3.2311, -1.8930,  4.7245],\n",
            "        [-2.3501, -1.2138,  3.1577],\n",
            "        [-3.0897, -2.2340,  4.8858],\n",
            "        [ 4.2872, -2.5723, -2.0680]], device='cuda:0')\n",
            "logits [[ 4.253812   -1.5244542  -2.6873102 ]\n",
            " [-1.8564762  -2.0075793   3.3819356 ]\n",
            " [-2.7608333  -2.440983    4.794318  ]\n",
            " [-1.2162439   3.530767   -1.52965   ]\n",
            " [-3.3801782  -1.7989349   4.891588  ]\n",
            " [-0.595856   -1.297023    1.6335435 ]\n",
            " [-0.7966231   3.6898518  -2.0602329 ]\n",
            " [ 4.3017035  -1.2599367  -3.10657   ]\n",
            " [-3.1093001  -2.2131863   4.876153  ]\n",
            " [ 1.1767119  -0.89674747 -0.21817453]\n",
            " [ 4.4993405  -2.5955439  -2.2218268 ]\n",
            " [-2.859211   -2.351674    4.7580175 ]\n",
            " [-3.2311387  -1.8930453   4.724522  ]\n",
            " [-2.3501053  -1.213836    3.1577163 ]\n",
            " [-3.0896678  -2.2339616   4.8857937 ]\n",
            " [ 4.2872105  -2.5723116  -2.0680115 ]]\n",
            "label_ids [0 0 2 1 2 2 2 0 2 0 0 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  1045,  3811,  ...,     0,     0,     0],\n",
            "        [  101,  2025,  2069,  ...,     0,     0,     0],\n",
            "        [  101,  2017,  2442,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  4664,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 25545,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2173,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2], device='cuda:0')\n",
            "logits tensor([[-2.8506, -2.4787,  4.8510],\n",
            "        [-2.9396, -2.4011,  4.9146],\n",
            "        [-2.7378, -2.4608,  4.7380],\n",
            "        [-3.1614, -1.3683,  4.3280],\n",
            "        [-2.8620, -2.4299,  4.8499],\n",
            "        [ 4.7439, -2.2778, -2.6156],\n",
            "        [-3.1963, -1.0624,  4.0890],\n",
            "        [ 3.3871, -1.2832, -2.1562],\n",
            "        [-2.9590, -1.9977,  4.4865],\n",
            "        [-2.9682, -2.0464,  4.4923],\n",
            "        [-3.3120, -1.9771,  4.8899],\n",
            "        [-3.3368, -1.8497,  4.9159],\n",
            "        [ 5.0080, -2.1701, -3.0243],\n",
            "        [-3.3809, -1.8618,  4.8837],\n",
            "        [ 1.6258,  0.0530, -1.5588],\n",
            "        [-2.6736, -2.5184,  4.7789]], device='cuda:0')\n",
            "logits [[-2.8506384  -2.478721    4.851038  ]\n",
            " [-2.9395714  -2.4010673   4.9146047 ]\n",
            " [-2.7377934  -2.4608054   4.737991  ]\n",
            " [-3.1613984  -1.3683143   4.328005  ]\n",
            " [-2.8619602  -2.429884    4.8499446 ]\n",
            " [ 4.7439046  -2.277822   -2.6155899 ]\n",
            " [-3.196301   -1.0623599   4.088998  ]\n",
            " [ 3.3871422  -1.2832438  -2.1561742 ]\n",
            " [-2.9590096  -1.9976976   4.486484  ]\n",
            " [-2.9682178  -2.0463848   4.4923134 ]\n",
            " [-3.3119874  -1.9770774   4.88992   ]\n",
            " [-3.336842   -1.8496622   4.9159174 ]\n",
            " [ 5.0080495  -2.1700673  -3.024282  ]\n",
            " [-3.380887   -1.8617848   4.8836665 ]\n",
            " [ 1.6258461   0.05303332 -1.5587515 ]\n",
            " [-2.6735969  -2.5184107   4.7789207 ]]\n",
            "label_ids [2 2 2 2 2 0 2 0 2 2 2 2 0 2 0 2]\n",
            "b_input_ids tensor([[  101,  1045,  8823,  ...,     0,     0,     0],\n",
            "        [  101,  2672,  2009,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  8855,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2326,  1010,  ...,     0,     0,     0],\n",
            "        [  101,  2026, 10861,  ...,     0,     0,     0],\n",
            "        [  101,  2027,  2024,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 0, 0, 2, 0, 0], device='cuda:0')\n",
            "logits tensor([[ 4.6141, -2.4141, -2.4763],\n",
            "        [-2.7478, -2.2235,  4.6181],\n",
            "        [-3.2363, -2.0808,  4.9240],\n",
            "        [-3.1006, -1.3628,  4.2573],\n",
            "        [-2.9727, -2.2466,  4.8654],\n",
            "        [ 0.1136, -0.2444, -0.0518],\n",
            "        [-3.2530, -2.0246,  4.9154],\n",
            "        [-2.9802, -2.3536,  4.8944],\n",
            "        [ 1.4145, -1.7129, -0.0268],\n",
            "        [ 4.7161, -2.3759, -2.5841],\n",
            "        [-2.4028, -2.1604,  4.1444],\n",
            "        [ 4.4305, -2.6187, -2.1941],\n",
            "        [-0.1281, -0.5618,  0.6132],\n",
            "        [-2.8425, -2.3935,  4.9224],\n",
            "        [ 4.6855, -2.4041, -2.4204],\n",
            "        [ 4.9760, -2.0939, -3.1426]], device='cuda:0')\n",
            "logits [[ 4.614076   -2.414133   -2.4762785 ]\n",
            " [-2.7477846  -2.223508    4.618079  ]\n",
            " [-3.2363076  -2.0808284   4.9240274 ]\n",
            " [-3.1005936  -1.3627905   4.2572966 ]\n",
            " [-2.9727497  -2.2465534   4.865363  ]\n",
            " [ 0.11358808 -0.24439462 -0.05182357]\n",
            " [-3.2529943  -2.0246358   4.915381  ]\n",
            " [-2.980188   -2.353577    4.894367  ]\n",
            " [ 1.4144808  -1.7129371  -0.02677088]\n",
            " [ 4.716052   -2.3758721  -2.5841124 ]\n",
            " [-2.4027636  -2.1604264   4.144429  ]\n",
            " [ 4.4305058  -2.6187153  -2.1940696 ]\n",
            " [-0.12814225 -0.5617573   0.61322767]\n",
            " [-2.8425386  -2.3935268   4.9223537 ]\n",
            " [ 4.685503   -2.4040751  -2.4203842 ]\n",
            " [ 4.9760375  -2.0939324  -3.1425984 ]]\n",
            "label_ids [0 2 2 2 2 2 2 2 0 1 0 0 0 2 0 0]\n",
            "b_input_ids tensor([[  101,  6293,  2007,  ...,     0,     0,     0],\n",
            "        [  101, 25545,  2003,  ...,     0,     0,     0],\n",
            "        [  101,  2307,  5379,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2065,  2017,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  3095,  ...,     0,     0,     0],\n",
            "        [  101,  7297,  1010,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[-2.8592, -2.3517,  4.7580],\n",
            "        [-2.2204,  0.1564,  2.1548],\n",
            "        [-3.4265, -1.7873,  4.8636],\n",
            "        [-2.9511, -2.3572,  4.8447],\n",
            "        [ 4.0301, -1.0094, -3.1254],\n",
            "        [-2.8469, -2.4507,  4.8163],\n",
            "        [ 4.3438, -1.2060, -3.2426],\n",
            "        [-2.8506, -2.4787,  4.8510],\n",
            "        [ 1.2985, -2.1780,  0.5066],\n",
            "        [-2.9470, -2.3216,  4.8245],\n",
            "        [ 3.7133, -2.6992, -1.5318],\n",
            "        [-1.7634, -1.3733,  2.7529],\n",
            "        [-3.0448, -2.2976,  4.8653],\n",
            "        [ 4.9301, -1.8508, -3.2614],\n",
            "        [-3.0220, -2.3160,  4.9415],\n",
            "        [ 4.8244, -2.5017, -2.5335]], device='cuda:0')\n",
            "logits [[-2.859211   -2.351674    4.7580175 ]\n",
            " [-2.2204258   0.15641737  2.1547546 ]\n",
            " [-3.4265096  -1.7873456   4.8635874 ]\n",
            " [-2.9510741  -2.3572078   4.8446817 ]\n",
            " [ 4.0300965  -1.009373   -3.1254427 ]\n",
            " [-2.8469276  -2.4506686   4.816306  ]\n",
            " [ 4.343805   -1.2060016  -3.2426329 ]\n",
            " [-2.8506384  -2.478721    4.851038  ]\n",
            " [ 1.2985309  -2.1779997   0.5066191 ]\n",
            " [-2.94696    -2.3215663   4.8244934 ]\n",
            " [ 3.7132654  -2.6991782  -1.531764  ]\n",
            " [-1.763405   -1.373322    2.7528508 ]\n",
            " [-3.044756   -2.2975736   4.865325  ]\n",
            " [ 4.930127   -1.8508339  -3.2613666 ]\n",
            " [-3.0219603  -2.3160458   4.9415164 ]\n",
            " [ 4.8243504  -2.501698   -2.533465  ]]\n",
            "label_ids [2 0 2 2 0 2 0 2 0 2 2 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  2079,  3489,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2018,  ...,     0,     0,     0],\n",
            "        [  101,  2293, 10733,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  8840,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2031,  ...,     0,     0,     0],\n",
            "        [  101,  2026,  2684,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[-2.8106, -2.4755,  4.7922],\n",
            "        [-2.9537, -2.0658,  4.6609],\n",
            "        [-3.0041, -2.1920,  4.8433],\n",
            "        [-1.1993,  0.1242,  1.1331],\n",
            "        [ 4.2464, -0.6174, -3.6270],\n",
            "        [ 4.7643, -2.4443, -2.4850],\n",
            "        [-3.0213, -2.2976,  4.8476],\n",
            "        [ 4.7463, -2.4545, -2.5018],\n",
            "        [ 4.2964, -1.8890, -2.5162],\n",
            "        [-3.0048, -1.9988,  4.7704],\n",
            "        [-1.5587, -1.7250,  2.7889],\n",
            "        [ 4.7133, -2.3161, -2.5785],\n",
            "        [ 4.6435, -2.5432, -2.3447],\n",
            "        [-2.8810, -2.4514,  4.8163],\n",
            "        [ 0.5339, -2.2802,  1.3354],\n",
            "        [-2.5200, -2.6887,  4.7392]], device='cuda:0')\n",
            "logits [[-2.8106418  -2.475523    4.7921906 ]\n",
            " [-2.9536955  -2.0657601   4.6608853 ]\n",
            " [-3.0041213  -2.1920485   4.843295  ]\n",
            " [-1.1992695   0.12419979  1.1331027 ]\n",
            " [ 4.246445   -0.61737376 -3.6269958 ]\n",
            " [ 4.7643404  -2.4442568  -2.4849527 ]\n",
            " [-3.0213273  -2.2975607   4.8475933 ]\n",
            " [ 4.7463183  -2.4545188  -2.5018103 ]\n",
            " [ 4.2964168  -1.8889599  -2.516211  ]\n",
            " [-3.004833   -1.9987738   4.770441  ]\n",
            " [-1.5587     -1.7250067   2.7888653 ]\n",
            " [ 4.7133155  -2.3160543  -2.5785244 ]\n",
            " [ 4.6435075  -2.5432458  -2.3446715 ]\n",
            " [-2.8809786  -2.4513526   4.816309  ]\n",
            " [ 0.533913   -2.2802243   1.3354192 ]\n",
            " [-2.5199866  -2.688655    4.739194  ]]\n",
            "label_ids [2 2 2 0 0 0 2 0 0 2 2 0 0 2 2 2]\n",
            "b_input_ids tensor([[ 101, 1996, 2833,  ...,    0,    0,    0],\n",
            "        [ 101, 2057, 2071,  ...,    0,    0,    0],\n",
            "        [ 101, 2023, 2173,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 4468, 2023,  ...,    0,    0,    0],\n",
            "        [ 101, 2673, 2001,  ...,    0,    0,    0],\n",
            "        [ 101, 2205, 2919,  ...,    0,    0,    0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[-2.7346, -2.5537,  4.8176],\n",
            "        [-1.8707, -0.8782,  2.6465],\n",
            "        [-0.1822, -1.1525,  1.2052],\n",
            "        [-2.8570, -2.4467,  4.8520],\n",
            "        [-3.1074, -2.1146,  4.8960],\n",
            "        [-2.7567, -2.5189,  4.7942],\n",
            "        [-0.5441, -1.0143,  1.3465],\n",
            "        [-2.6989, -2.5741,  4.8278],\n",
            "        [-2.9876, -2.2923,  4.9045],\n",
            "        [-3.0441, -2.2176,  4.8597],\n",
            "        [-3.0371, -2.1901,  4.8671],\n",
            "        [-3.0831, -2.2805,  4.9174],\n",
            "        [-3.1478, -2.0963,  4.8983],\n",
            "        [ 4.2129, -2.6421, -2.0567],\n",
            "        [-2.8812, -2.3950,  4.8697],\n",
            "        [ 4.5010, -1.2411, -3.4483]], device='cuda:0')\n",
            "logits [[-2.7345657  -2.5536761   4.8176136 ]\n",
            " [-1.8706697  -0.87822145  2.646478  ]\n",
            " [-0.18215786 -1.1524668   1.2052386 ]\n",
            " [-2.8569565  -2.4467149   4.852047  ]\n",
            " [-3.1074235  -2.1145592   4.895992  ]\n",
            " [-2.7567296  -2.5189013   4.7941823 ]\n",
            " [-0.54414433 -1.0142605   1.3464534 ]\n",
            " [-2.6989202  -2.5740676   4.8278265 ]\n",
            " [-2.9876184  -2.2922726   4.9044533 ]\n",
            " [-3.0440912  -2.2175522   4.8597426 ]\n",
            " [-3.037117   -2.1901436   4.86715   ]\n",
            " [-3.083114   -2.2805307   4.917404  ]\n",
            " [-3.1478481  -2.0963316   4.898315  ]\n",
            " [ 4.2129426  -2.6421094  -2.0566638 ]\n",
            " [-2.8811774  -2.3949533   4.869691  ]\n",
            " [ 4.5010443  -1.241115   -3.4482937 ]]\n",
            "label_ids [2 2 0 2 2 2 2 2 2 2 2 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  1996,  2833,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2001,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2326,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  2131,  1996,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 17917,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[-2.9470, -2.1328,  4.6925],\n",
            "        [-2.1397, -2.8199,  4.5350],\n",
            "        [ 4.7561, -2.5234, -2.4283],\n",
            "        [-3.2389, -2.0600,  4.9241],\n",
            "        [-2.4247, -2.7446,  4.6536],\n",
            "        [-2.0145, -2.3347,  3.8070],\n",
            "        [ 0.7179, -0.9202,  0.0451],\n",
            "        [-3.1218, -1.8672,  4.6642],\n",
            "        [-3.0782, -2.2537,  4.8727],\n",
            "        [-1.0901,  0.3174,  1.0771],\n",
            "        [-0.6235,  3.5078, -2.2914],\n",
            "        [-2.7601, -1.1351,  3.5988],\n",
            "        [ 3.3425, -1.8499, -1.7797],\n",
            "        [-3.0975, -2.2229,  4.9010],\n",
            "        [-3.2424, -2.0157,  4.8671],\n",
            "        [ 4.7822, -1.9698, -2.9201]], device='cuda:0')\n",
            "logits [[-2.9469705  -2.1328354   4.6924777 ]\n",
            " [-2.1397452  -2.8198786   4.5349774 ]\n",
            " [ 4.756149   -2.5233643  -2.428319  ]\n",
            " [-3.2389019  -2.0600452   4.924106  ]\n",
            " [-2.4247222  -2.7446237   4.6535773 ]\n",
            " [-2.0145326  -2.3346753   3.8070176 ]\n",
            " [ 0.71789575 -0.9201711   0.04507988]\n",
            " [-3.1218128  -1.8672119   4.6642265 ]\n",
            " [-3.0782354  -2.2536826   4.872717  ]\n",
            " [-1.0900964   0.31741875  1.0771087 ]\n",
            " [-0.623468    3.5077808  -2.2913704 ]\n",
            " [-2.7601345  -1.1350926   3.598774  ]\n",
            " [ 3.3424952  -1.8498803  -1.7796767 ]\n",
            " [-3.0974586  -2.2229214   4.9010177 ]\n",
            " [-3.2424483  -2.0156822   4.8670955 ]\n",
            " [ 4.7821507  -1.9697632  -2.9200642 ]]\n",
            "label_ids [2 2 0 2 2 2 0 1 2 0 0 2 2 2 2 2]\n",
            "b_input_ids tensor([[ 101, 2065, 2017,  ...,    0,    0,    0],\n",
            "        [ 101, 2000, 2022,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 8840,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2204, 2833,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 2686,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 2326,  ...,    0,    0,    0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[ 4.6213, -2.6132, -2.3251],\n",
            "        [-1.2830, -2.0951,  2.7269],\n",
            "        [-2.8281, -2.4699,  4.7981],\n",
            "        [ 0.1522, -2.0284,  1.2329],\n",
            "        [-2.9337, -2.0110,  4.5684],\n",
            "        [ 0.2646, -0.1074, -0.1361],\n",
            "        [-3.0445, -2.2610,  4.9236],\n",
            "        [ 3.8155, -2.2772, -1.9416],\n",
            "        [ 4.7938, -2.3128, -2.6851],\n",
            "        [-2.7604, -1.4439,  3.8580],\n",
            "        [ 4.7421, -2.4910, -2.5296],\n",
            "        [-0.5741,  0.6870,  0.0713],\n",
            "        [-3.2433, -2.0148,  4.9123],\n",
            "        [-2.5216,  0.0065,  2.5200],\n",
            "        [ 3.4627, -1.6510, -1.8891],\n",
            "        [-3.0582, -2.2708,  4.9254]], device='cuda:0')\n",
            "logits [[ 4.621309   -2.6132238  -2.325088  ]\n",
            " [-1.283034   -2.0950503   2.7269328 ]\n",
            " [-2.828126   -2.4698715   4.7981324 ]\n",
            " [ 0.15224923 -2.0283587   1.2328804 ]\n",
            " [-2.9336927  -2.0110424   4.568394  ]\n",
            " [ 0.26455417 -0.10743837 -0.13611838]\n",
            " [-3.0445142  -2.2609625   4.92364   ]\n",
            " [ 3.8154554  -2.2772098  -1.9415796 ]\n",
            " [ 4.79385    -2.3127646  -2.6850622 ]\n",
            " [-2.7604108  -1.4438567   3.858025  ]\n",
            " [ 4.7420626  -2.4910026  -2.529589  ]\n",
            " [-0.5741274   0.68704575  0.07126129]\n",
            " [-3.2433434  -2.0148182   4.912274  ]\n",
            " [-2.521636    0.00647452  2.5200174 ]\n",
            " [ 3.4626553  -1.6509657  -1.8891194 ]\n",
            " [-3.0582418  -2.2707915   4.925394  ]]\n",
            "label_ids [0 0 2 2 2 2 2 0 0 2 0 2 2 2 2 2]\n",
            "b_input_ids tensor([[  101,  1996,  2173,  2003,  1037, 20377, 13181,  2029,  2965,  1024,\n",
            "          3722, 10447,  1998,  4511,  2366, 18228,  1999,  1037, 13950,  2989,\n",
            "          7224,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  3071, 23289,  2094,  2055,  1996,  7224,  1006, 11552,  4734,\n",
            "          1998,  7078,  4297, 25377, 25236,  5328,  1007,  1998,  1996, 18783,\n",
            "          2833,   999,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1996,  2833,  2001, 19960,  3695, 16748,  2012,  2190,  2021,\n",
            "          2009,  2001,  1996,  9202,  2326,  2008,  2081,  2033, 19076,  2196,\n",
            "          2000,  2175,  2067,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2023, 24209, 22325,  1998,  6298, 19817, 19321, 11069,  2003,\n",
            "          2012,  1996,  2327,  1997,  2026,  7128,  4825,  2862,  1012,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2031,  1996, 28248,  5572,  1012,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2307,  2833,  1010,  2307, 25545,  1010,  2307,  2326,  1012,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1045,  2031,  2042,  2000, 12211,  1005,  1055,  3807,  1998,\n",
            "          2119,  2335,  2020,  2200, 15640,  1012,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "       device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  75%|███████▌  | 3/4 [01:14<00:24, 24.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 0], device='cuda:0')\n",
            "logits tensor([[-3.0606, -2.2685,  4.8824],\n",
            "        [-2.9309, -2.3608,  4.9390],\n",
            "        [ 4.9138, -2.0726, -3.0191],\n",
            "        [-2.8243, -2.4677,  4.8309],\n",
            "        [-3.4233, -1.2087,  4.4446],\n",
            "        [-3.2389, -2.0600,  4.9241],\n",
            "        [ 3.8477, -2.8061, -1.6070]], device='cuda:0')\n",
            "logits [[-3.0606444 -2.2685306  4.8824296]\n",
            " [-2.930889  -2.3608398  4.9389944]\n",
            " [ 4.913752  -2.0725923 -3.0190537]\n",
            " [-2.8242502 -2.4676895  4.830896 ]\n",
            " [-3.4233007 -1.2087255  4.444578 ]\n",
            " [-3.2389019 -2.0600452  4.924106 ]\n",
            " [ 3.8476508 -2.8060827 -1.6070278]]\n",
            "label_ids [2 2 0 2 2 2 0]\n",
            "Validation Accuracy: 0.85\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.3473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.2052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "Train loss: 0.094332209485583\n",
            "b_input_ids tensor([[  101,  1996, 13877,  ...,     0,     0,     0],\n",
            "        [  101,  2061,  2054,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2326,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2123,  1005,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2069,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0], device='cuda:0')\n",
            "logits tensor([[ 4.2750, -2.6842, -1.9063],\n",
            "        [ 3.5627, -1.7016, -1.8407],\n",
            "        [-2.6665, -2.7932,  4.9523],\n",
            "        [-2.9688, -2.6192,  5.1158],\n",
            "        [-2.8056, -2.7596,  5.0547],\n",
            "        [-2.6331, -2.7330,  4.9143],\n",
            "        [-2.5581, -2.8590,  4.9562],\n",
            "        [ 4.1800, -0.7182, -3.6975],\n",
            "        [-3.0330, -2.4937,  5.0905],\n",
            "        [-2.9588, -2.6537,  5.0703],\n",
            "        [-2.8428, -2.7430,  5.0269],\n",
            "        [ 0.5442,  0.9114, -1.0981],\n",
            "        [-1.7514, -2.5181,  3.9559],\n",
            "        [ 2.3952, -1.7456, -0.7909],\n",
            "        [-2.0928, -3.1215,  4.6977],\n",
            "        [-0.2620, -0.0898,  0.6060]], device='cuda:0')\n",
            "logits [[ 4.275013   -2.6841872  -1.9063277 ]\n",
            " [ 3.5626514  -1.7016201  -1.8406577 ]\n",
            " [-2.666464   -2.7931876   4.952269  ]\n",
            " [-2.9688363  -2.6192472   5.115782  ]\n",
            " [-2.8055978  -2.759576    5.0547366 ]\n",
            " [-2.6330533  -2.7329843   4.9143023 ]\n",
            " [-2.5580728  -2.8590362   4.9561977 ]\n",
            " [ 4.179997   -0.7182083  -3.6975327 ]\n",
            " [-3.0330489  -2.4936936   5.0904946 ]\n",
            " [-2.958838   -2.6536927   5.070347  ]\n",
            " [-2.842823   -2.742983    5.0269246 ]\n",
            " [ 0.54424983  0.9114469  -1.0981394 ]\n",
            " [-1.7513752  -2.5181046   3.955862  ]\n",
            " [ 2.3952081  -1.745609   -0.7909102 ]\n",
            " [-2.092822   -3.1215034   4.6976533 ]\n",
            " [-0.26202825 -0.0898298   0.6059881 ]]\n",
            "label_ids [0 1 2 2 2 2 2 0 2 2 2 2 2 2 2 0]\n",
            "b_input_ids tensor([[  101, 13558, 18064,  ...,     0,     0,     0],\n",
            "        [  101,  1037,  2367,  ...,     0,     0,     0],\n",
            "        [  101,  2296,  2051,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  2173,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2428,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 18499,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 0, 2, 1, 2, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[ 3.6674, -0.2641, -3.3736],\n",
            "        [-2.5104, -2.7561,  4.7059],\n",
            "        [-2.6194, -2.8197,  4.9344],\n",
            "        [-1.4018,  3.5627, -1.4229],\n",
            "        [-3.0217, -2.5706,  5.1186],\n",
            "        [-1.7593, -2.4148,  3.5864],\n",
            "        [-1.1846,  3.7309, -1.6809],\n",
            "        [ 3.5307, -0.8728, -2.5295],\n",
            "        [-2.9685, -2.6609,  5.0632],\n",
            "        [ 0.4191, -0.8133,  0.4639],\n",
            "        [ 3.0577, -2.9826, -0.7544],\n",
            "        [-2.6511, -2.8102,  4.9175],\n",
            "        [-2.8172, -2.7347,  4.9944],\n",
            "        [-3.0468, -2.5005,  5.0751],\n",
            "        [-2.9660, -2.6599,  5.0767],\n",
            "        [ 4.2530, -2.4798, -2.0549]], device='cuda:0')\n",
            "logits [[ 3.6674411  -0.26406682 -3.3736339 ]\n",
            " [-2.510364   -2.7561362   4.705893  ]\n",
            " [-2.6193843  -2.819697    4.934434  ]\n",
            " [-1.4017545   3.5627306  -1.4228861 ]\n",
            " [-3.0216808  -2.570574    5.1185513 ]\n",
            " [-1.759263   -2.414758    3.5864236 ]\n",
            " [-1.1846125   3.730883   -1.6808678 ]\n",
            " [ 3.53067    -0.8728066  -2.5294971 ]\n",
            " [-2.9684675  -2.6608543   5.0631633 ]\n",
            " [ 0.41910484 -0.813344    0.46388572]\n",
            " [ 3.0577307  -2.9826348  -0.7544238 ]\n",
            " [-2.6511068  -2.810181    4.917501  ]\n",
            " [-2.817216   -2.7346902   4.9943576 ]\n",
            " [-3.0468256  -2.5004747   5.0750823 ]\n",
            " [-2.966044   -2.6599135   5.0767198 ]\n",
            " [ 4.252998   -2.4797666  -2.0549078 ]]\n",
            "label_ids [0 0 2 1 2 2 2 0 2 0 0 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  1045,  3811,  ...,     0,     0,     0],\n",
            "        [  101,  2025,  2069,  ...,     0,     0,     0],\n",
            "        [  101,  2017,  2442,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  4664,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 25545,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2173,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2], device='cuda:0')\n",
            "logits tensor([[-2.7309, -2.8443,  5.0053],\n",
            "        [-2.7194, -2.8195,  5.0236],\n",
            "        [-2.5676, -2.8596,  4.8713],\n",
            "        [-3.0226, -2.2801,  4.9351],\n",
            "        [-2.7140, -2.8272,  5.0004],\n",
            "        [ 4.2572, -2.1399, -2.1159],\n",
            "        [-3.1012, -2.4514,  5.0927],\n",
            "        [ 2.5554, -1.6575, -0.8457],\n",
            "        [-2.8817, -2.6879,  4.9971],\n",
            "        [-2.8768, -2.6513,  4.8765],\n",
            "        [-2.9588, -2.6537,  5.0703],\n",
            "        [-3.0140, -2.5732,  5.1331],\n",
            "        [ 4.9055, -2.0843, -2.9878],\n",
            "        [-3.1038, -2.5065,  5.0987],\n",
            "        [ 0.8763, -0.5445, -0.2468],\n",
            "        [-2.5737, -2.8575,  4.9276]], device='cuda:0')\n",
            "logits [[-2.7308812  -2.8443248   5.005264  ]\n",
            " [-2.719439   -2.8194644   5.023562  ]\n",
            " [-2.567594   -2.8596005   4.8712945 ]\n",
            " [-3.0226345  -2.280132    4.935147  ]\n",
            " [-2.7140446  -2.8272357   5.0003533 ]\n",
            " [ 4.2572474  -2.1399498  -2.1159317 ]\n",
            " [-3.1012263  -2.4514298   5.092691  ]\n",
            " [ 2.55544    -1.6574717  -0.84565395]\n",
            " [-2.88166    -2.68787     4.997051  ]\n",
            " [-2.8767638  -2.651253    4.87645   ]\n",
            " [-2.958838   -2.6536927   5.070347  ]\n",
            " [-3.01399    -2.5732431   5.1330943 ]\n",
            " [ 4.905458   -2.084304   -2.9878378 ]\n",
            " [-3.1038153  -2.5064743   5.098661  ]\n",
            " [ 0.8762886  -0.54450315 -0.24677648]\n",
            " [-2.573673   -2.857531    4.927584  ]]\n",
            "label_ids [2 2 2 2 2 0 2 0 2 2 2 2 0 2 0 2]\n",
            "b_input_ids tensor([[  101,  1045,  8823,  ...,     0,     0,     0],\n",
            "        [  101,  2672,  2009,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  8855,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2326,  1010,  ...,     0,     0,     0],\n",
            "        [  101,  2026, 10861,  ...,     0,     0,     0],\n",
            "        [  101,  2027,  2024,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 0, 0, 2, 0, 0], device='cuda:0')\n",
            "logits tensor([[ 2.7404, -2.5265, -0.7521],\n",
            "        [-2.4610, -2.8749,  4.8894],\n",
            "        [-2.9931, -2.6277,  5.0940],\n",
            "        [-3.0834, -2.1188,  4.8108],\n",
            "        [-2.8895, -2.6605,  5.0722],\n",
            "        [-1.1409, -2.1923,  2.5362],\n",
            "        [-2.9829, -2.6108,  5.1041],\n",
            "        [-2.7819, -2.8049,  5.0286],\n",
            "        [ 1.1622, -2.6092,  0.7807],\n",
            "        [ 4.2740, -2.6437, -1.9905],\n",
            "        [-2.4274, -2.8731,  4.8427],\n",
            "        [ 3.0142, -3.1511, -0.6044],\n",
            "        [-0.2883, -1.6919,  1.4883],\n",
            "        [-2.6587, -2.8204,  5.0402],\n",
            "        [ 4.3537, -2.7205, -1.9287],\n",
            "        [ 4.7246, -1.6066, -3.4010]], device='cuda:0')\n",
            "logits [[ 2.7403567  -2.5265012  -0.75205153]\n",
            " [-2.4609606  -2.8749335   4.8893523 ]\n",
            " [-2.9931295  -2.6276991   5.094043  ]\n",
            " [-3.0833895  -2.1188266   4.81077   ]\n",
            " [-2.8894563  -2.6604517   5.072176  ]\n",
            " [-1.1409091  -2.1923244   2.5362358 ]\n",
            " [-2.9828582  -2.6107981   5.1041255 ]\n",
            " [-2.7819333  -2.8048954   5.0285873 ]\n",
            " [ 1.1622486  -2.6091726   0.78072757]\n",
            " [ 4.2740335  -2.6436508  -1.9904828 ]\n",
            " [-2.4274418  -2.8731496   4.8426986 ]\n",
            " [ 3.014173   -3.1510952  -0.6044263 ]\n",
            " [-0.28832993 -1.6918563   1.4882818 ]\n",
            " [-2.6586702  -2.8203995   5.0401993 ]\n",
            " [ 4.353668   -2.7204733  -1.9286517 ]\n",
            " [ 4.724557   -1.6065887  -3.4009838 ]]\n",
            "label_ids [0 2 2 2 2 2 2 2 0 1 0 0 0 2 0 0]\n",
            "b_input_ids tensor([[  101,  6293,  2007,  ...,     0,     0,     0],\n",
            "        [  101, 25545,  2003,  ...,     0,     0,     0],\n",
            "        [  101,  2307,  5379,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2065,  2017,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  3095,  ...,     0,     0,     0],\n",
            "        [  101,  7297,  1010,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[-2.6511, -2.8102,  4.9175],\n",
            "        [-3.0911, -1.5398,  4.2860],\n",
            "        [-3.1209, -2.5079,  5.0980],\n",
            "        [-2.8428, -2.7430,  5.0269],\n",
            "        [ 1.9003, -1.1205, -0.7839],\n",
            "        [-2.7466, -2.8199,  4.9946],\n",
            "        [ 3.9627, -0.9256, -3.1357],\n",
            "        [-2.7309, -2.8443,  5.0053],\n",
            "        [ 1.0652, -2.8780,  1.1547],\n",
            "        [-2.7887, -2.7487,  5.0005],\n",
            "        [ 3.2032, -3.2099, -0.7160],\n",
            "        [-2.7043, -2.7006,  4.9267],\n",
            "        [-2.7286, -2.8267,  4.9857],\n",
            "        [ 4.2202, -0.7229, -3.7281],\n",
            "        [-2.7931, -2.7851,  5.0606],\n",
            "        [ 4.5739, -1.5044, -3.4038]], device='cuda:0')\n",
            "logits [[-2.6511068  -2.810181    4.917501  ]\n",
            " [-3.091055   -1.5397561   4.2859683 ]\n",
            " [-3.1209002  -2.5079036   5.0979657 ]\n",
            " [-2.842823   -2.742983    5.0269246 ]\n",
            " [ 1.9003     -1.1204847  -0.7839177 ]\n",
            " [-2.7465706  -2.8199034   4.994633  ]\n",
            " [ 3.9627442  -0.9256158  -3.1356966 ]\n",
            " [-2.7308812  -2.8443248   5.005264  ]\n",
            " [ 1.0651823  -2.878002    1.1547291 ]\n",
            " [-2.788671   -2.7486966   5.0005245 ]\n",
            " [ 3.2032056  -3.209933   -0.7160428 ]\n",
            " [-2.704273   -2.7006407   4.926665  ]\n",
            " [-2.7286232  -2.8267329   4.9857354 ]\n",
            " [ 4.2202     -0.72293764 -3.7281182 ]\n",
            " [-2.793097   -2.7850716   5.060632  ]\n",
            " [ 4.573891   -1.5044112  -3.4038155 ]]\n",
            "label_ids [2 0 2 2 0 2 0 2 0 2 2 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  2079,  3489,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2018,  ...,     0,     0,     0],\n",
            "        [  101,  2293, 10733,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1996,  8840,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2031,  ...,     0,     0,     0],\n",
            "        [  101,  2026,  2684,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[-2.6497, -2.8603,  4.9309],\n",
            "        [-2.8360, -2.6629,  5.0176],\n",
            "        [-2.9149, -2.6185,  5.0627],\n",
            "        [-2.1004, -1.2905,  3.0034],\n",
            "        [ 2.3144,  1.1783, -3.1730],\n",
            "        [ 4.4686, -1.3766, -3.3777],\n",
            "        [-2.8685, -2.7281,  5.0280],\n",
            "        [ 4.4403, -2.6737, -2.0910],\n",
            "        [ 3.1970, -2.3613, -1.1612],\n",
            "        [-2.7498, -2.6640,  5.0427],\n",
            "        [-2.2776, -2.7681,  4.5203],\n",
            "        [ 4.4370, -2.4294, -2.2334],\n",
            "        [ 4.3384, -1.0997, -3.4789],\n",
            "        [-2.7157, -2.8414,  4.9635],\n",
            "        [ 0.0975, -3.0029,  2.3581],\n",
            "        [-2.4493, -2.9812,  4.8841]], device='cuda:0')\n",
            "logits [[-2.649655   -2.860288    4.9308743 ]\n",
            " [-2.8360095  -2.6629338   5.0176253 ]\n",
            " [-2.9148645  -2.6185212   5.062688  ]\n",
            " [-2.1004303  -1.2904681   3.0034375 ]\n",
            " [ 2.3143542   1.1783266  -3.1730118 ]\n",
            " [ 4.468568   -1.376624   -3.3776708 ]\n",
            " [-2.8684893  -2.728121    5.0279613 ]\n",
            " [ 4.440314   -2.6737368  -2.0909915 ]\n",
            " [ 3.1969998  -2.3613145  -1.1612359 ]\n",
            " [-2.7498474  -2.6640215   5.042706  ]\n",
            " [-2.277629   -2.768112    4.5202546 ]\n",
            " [ 4.436964   -2.4294062  -2.2333603 ]\n",
            " [ 4.338392   -1.0997175  -3.4788988 ]\n",
            " [-2.7156668  -2.841433    4.963484  ]\n",
            " [ 0.09751824 -3.0028627   2.3580687 ]\n",
            " [-2.4493372  -2.9811862   4.884127  ]]\n",
            "label_ids [2 2 2 0 0 0 2 0 0 2 2 0 0 2 2 2]\n",
            "b_input_ids tensor([[ 101, 1996, 2833,  ...,    0,    0,    0],\n",
            "        [ 101, 2057, 2071,  ...,    0,    0,    0],\n",
            "        [ 101, 2023, 2173,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 4468, 2023,  ...,    0,    0,    0],\n",
            "        [ 101, 2673, 2001,  ...,    0,    0,    0],\n",
            "        [ 101, 2205, 2919,  ...,    0,    0,    0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0], device='cuda:0')\n",
            "logits tensor([[-2.5848, -2.9320,  4.9684],\n",
            "        [-2.6130, -2.4081,  4.5274],\n",
            "        [-1.1252, -2.5030,  3.1824],\n",
            "        [-2.6611, -2.8722,  4.9767],\n",
            "        [-2.8223, -2.6971,  5.0594],\n",
            "        [-2.5379, -2.9306,  4.9016],\n",
            "        [-1.0318, -2.1721,  2.5515],\n",
            "        [-2.5571, -2.9340,  4.9469],\n",
            "        [-2.7958, -2.7449,  5.0516],\n",
            "        [-2.9024, -2.6602,  5.0503],\n",
            "        [-2.7598, -2.7485,  5.0269],\n",
            "        [-2.8433, -2.7635,  5.0544],\n",
            "        [-2.8817, -2.6716,  5.0757],\n",
            "        [ 4.6128, -2.1130, -2.7924],\n",
            "        [-2.6926, -2.8251,  5.0075],\n",
            "        [ 3.3348, -0.3225, -3.0461]], device='cuda:0')\n",
            "logits [[-2.5847716 -2.9319708  4.968359 ]\n",
            " [-2.6130314 -2.4081068  4.527407 ]\n",
            " [-1.1251581 -2.5030124  3.182404 ]\n",
            " [-2.6611428 -2.8722088  4.9767327]\n",
            " [-2.8222933 -2.6970713  5.059363 ]\n",
            " [-2.5378742 -2.9306047  4.901585 ]\n",
            " [-1.0317808 -2.1720614  2.551468 ]\n",
            " [-2.5570905 -2.933954   4.946938 ]\n",
            " [-2.795784  -2.7448695  5.0515933]\n",
            " [-2.9023855 -2.6601632  5.0502887]\n",
            " [-2.759756  -2.7484877  5.0269227]\n",
            " [-2.8433003 -2.7635381  5.0543833]\n",
            " [-2.8816996 -2.6716468  5.075702 ]\n",
            " [ 4.6127977 -2.1130464 -2.792411 ]\n",
            " [-2.6926084 -2.8251078  5.007493 ]\n",
            " [ 3.3348055 -0.3224534 -3.0461307]]\n",
            "label_ids [2 2 0 2 2 2 2 2 2 2 2 2 2 0 2 0]\n",
            "b_input_ids tensor([[  101,  1996,  2833,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2001,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2326,  2001,  ...,     0,     0,     0],\n",
            "        [  101,  2131,  1996,  ...,     0,     0,     0],\n",
            "        [  101,  1996, 17917,  ...,     0,     0,     0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[-2.5267, -2.8131,  4.8364],\n",
            "        [-2.0928, -3.1215,  4.6977],\n",
            "        [ 4.8568, -1.9147, -3.2259],\n",
            "        [-2.9975, -2.6176,  5.0933],\n",
            "        [-2.3062, -3.0785,  4.7807],\n",
            "        [-2.0009, -2.9332,  4.3067],\n",
            "        [ 0.5682, -1.8759,  0.9358],\n",
            "        [-2.7672, -2.6533,  4.9830],\n",
            "        [-2.9070, -2.6955,  5.0551],\n",
            "        [-2.5515, -0.9470,  3.2234],\n",
            "        [-0.5922,  3.5992, -2.4193],\n",
            "        [-2.8833, -2.1302,  4.5887],\n",
            "        [ 3.2906, -2.6409, -1.2429],\n",
            "        [-2.8133, -2.7640,  5.0350],\n",
            "        [-3.0397, -2.5650,  5.0779],\n",
            "        [ 4.5750, -1.6613, -2.9707]], device='cuda:0')\n",
            "logits [[-2.5266733  -2.8131173   4.836364  ]\n",
            " [-2.092822   -3.1215034   4.6976533 ]\n",
            " [ 4.856833   -1.914692   -3.225861  ]\n",
            " [-2.9975035  -2.61759     5.093325  ]\n",
            " [-2.3062186  -3.078468    4.78068   ]\n",
            " [-2.0008972  -2.933247    4.3067336 ]\n",
            " [ 0.56815046 -1.8759418   0.9357884 ]\n",
            " [-2.7672145  -2.6533096   4.9829993 ]\n",
            " [-2.9070282  -2.6955142   5.0551076 ]\n",
            " [-2.551503   -0.9470235   3.2234447 ]\n",
            " [-0.59215283  3.599239   -2.4193182 ]\n",
            " [-2.8832982  -2.130245    4.5887074 ]\n",
            " [ 3.2906327  -2.6408904  -1.2429006 ]\n",
            " [-2.813321   -2.7640078   5.0350294 ]\n",
            " [-3.0396678  -2.5649517   5.0779195 ]\n",
            " [ 4.5749927  -1.6612985  -2.9707475 ]]\n",
            "label_ids [2 2 0 2 2 2 0 1 2 0 0 2 2 2 2 2]\n",
            "b_input_ids tensor([[ 101, 2065, 2017,  ...,    0,    0,    0],\n",
            "        [ 101, 2000, 2022,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 8840,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2204, 2833,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 2686,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 2326,  ...,    0,    0,    0]], device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "logits tensor([[ 4.1512, -2.9986, -1.6459],\n",
            "        [-1.8934, -3.0283,  4.1993],\n",
            "        [-2.7160, -2.8287,  4.9686],\n",
            "        [-0.5759, -2.7481,  2.4270],\n",
            "        [-2.5877, -2.7161,  4.8286],\n",
            "        [-0.3881, -1.2897,  1.3810],\n",
            "        [-2.8361, -2.7097,  5.0537],\n",
            "        [ 3.2924, -2.9428, -1.1633],\n",
            "        [ 4.5732, -1.9760, -2.7549],\n",
            "        [-2.6894, -2.5792,  4.7970],\n",
            "        [ 4.8317, -2.4309, -2.6692],\n",
            "        [-0.6020, -0.0070,  0.8456],\n",
            "        [-2.9535, -2.6233,  5.0929],\n",
            "        [-3.1997, -2.2404,  4.9422],\n",
            "        [ 2.4291, -2.4654, -0.4548],\n",
            "        [-2.8274, -2.7489,  5.0551]], device='cuda:0')\n",
            "logits [[ 4.1511803  -2.9986465  -1.6458626 ]\n",
            " [-1.8933687  -3.0282893   4.199317  ]\n",
            " [-2.7160187  -2.828666    4.9686494 ]\n",
            " [-0.57586795 -2.7480786   2.4270482 ]\n",
            " [-2.5877445  -2.716064    4.8286095 ]\n",
            " [-0.38813528 -1.2897358   1.3810028 ]\n",
            " [-2.8361132  -2.7097065   5.0536714 ]\n",
            " [ 3.292389   -2.9427683  -1.1633135 ]\n",
            " [ 4.573214   -1.9759791  -2.7548614 ]\n",
            " [-2.6894212  -2.579231    4.7970405 ]\n",
            " [ 4.831652   -2.4308898  -2.6692445 ]\n",
            " [-0.6019706  -0.00697567  0.845613  ]\n",
            " [-2.9535115  -2.623275    5.0929375 ]\n",
            " [-3.199713   -2.240358    4.942181  ]\n",
            " [ 2.429062   -2.4653895  -0.45482853]\n",
            " [-2.8273501  -2.74891     5.0550804 ]]\n",
            "label_ids [0 0 2 2 2 2 2 0 0 2 0 2 2 2 2 2]\n",
            "b_input_ids tensor([[  101,  1996,  2173,  2003,  1037, 20377, 13181,  2029,  2965,  1024,\n",
            "          3722, 10447,  1998,  4511,  2366, 18228,  1999,  1037, 13950,  2989,\n",
            "          7224,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  3071, 23289,  2094,  2055,  1996,  7224,  1006, 11552,  4734,\n",
            "          1998,  7078,  4297, 25377, 25236,  5328,  1007,  1998,  1996, 18783,\n",
            "          2833,   999,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1996,  2833,  2001, 19960,  3695, 16748,  2012,  2190,  2021,\n",
            "          2009,  2001,  1996,  9202,  2326,  2008,  2081,  2033, 19076,  2196,\n",
            "          2000,  2175,  2067,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2023, 24209, 22325,  1998,  6298, 19817, 19321, 11069,  2003,\n",
            "          2012,  1996,  2327,  1997,  2026,  7128,  4825,  2862,  1012,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2031,  1996, 28248,  5572,  1012,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2307,  2833,  1010,  2307, 25545,  1010,  2307,  2326,  1012,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1045,  2031,  2042,  2000, 12211,  1005,  1055,  3807,  1998,\n",
            "          2119,  2335,  2020,  2200, 15640,  1012,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "       device='cuda:0') \n",
            " b_input_attention_mask tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.]], device='cuda:0') \n",
            " b_input_segment_mask "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 100%|██████████| 4/4 [01:39<00:00, 24.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0') \n",
            " b_labels tensor([2, 2, 0, 2, 2, 2, 0], device='cuda:0')\n",
            "logits tensor([[-2.8378, -2.7420,  5.0416],\n",
            "        [-2.7811, -2.7702,  5.0861],\n",
            "        [ 4.4922, -2.1693, -2.4971],\n",
            "        [-2.7352, -2.8086,  4.9956],\n",
            "        [-3.2689, -2.0986,  4.9740],\n",
            "        [-2.9975, -2.6176,  5.0933],\n",
            "        [ 2.3524, -3.2086,  0.0563]], device='cuda:0')\n",
            "logits [[-2.8378131 -2.7419705  5.0415835]\n",
            " [-2.7811115 -2.7702014  5.086135 ]\n",
            " [ 4.49225   -2.169306  -2.4970574]\n",
            " [-2.7351594 -2.8085597  4.995566 ]\n",
            " [-3.2688894 -2.0986283  4.973962 ]\n",
            " [-2.9975035 -2.61759    5.093325 ]\n",
            " [ 2.3524153 -3.208614   0.0563029]]\n",
            "label_ids [2 2 0 2 2 2 0]\n",
            "Validation Accuracy: 0.85\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDQIBqNX_8qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}