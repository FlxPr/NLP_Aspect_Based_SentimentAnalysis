{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "devdata = pd.read_csv('data/devdata.csv', sep = '\\t', header = None)\n",
    "traindata = pd.read_csv('data/traindata.csv', sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic preprocessing\n",
    "def preprocessing(df, nlp):\n",
    "\n",
    "    # rename columns\n",
    "    df = df.rename(columns = {0: 'sentiment', 1: 'aspect', 2: 'target', 3: 'character', 4: 'sentence'})\n",
    "    # define beginning and ending characters\n",
    "    df['begin'] = df['character'].apply(lambda x : int(x.split(\":\")[0]))\n",
    "    df['end'] = df['character'].apply(lambda x : int(x.split(\":\")[1]))\n",
    "\n",
    "    # lower cases \n",
    "    df['sentence'] = df['sentence'].apply(lambda x : x.lower())\n",
    "    \n",
    "    # remove punctuation\n",
    "    df['sentence'] = df['sentence'].apply(lambda x: nlp(x))\n",
    "    df['sentence'] = df['sentence'].apply(lambda x: [token.orth_ for token in x if not token.is_punct])\n",
    "    \n",
    "    # rejoin\n",
    "    df['sentence'] = df['sentence'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "devdata_df = preprocessing(devdata, nlp)\n",
    "traindata_df = preprocessing(traindata, nlp)\n",
    "\n",
    "# labels = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Hybrid Classification\n",
    "1. Detecting aspect terms \n",
    "    * training on liblinear\n",
    "2. Corresponding polarity\n",
    "    * classifier\n",
    "3. Detecting the aspect categories\n",
    "    * classifier\n",
    "4. Corresponding polarities\n",
    "    * classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Aspect Category Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-pretrained-bert pytorch-nlp\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing using BERT BasicTokeniser or BertTokeniser\n",
    "def preprocessing_BERT(df):\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def prep_BERT(df):\n",
    "    # sentence preparation (BERT standards)\n",
    "    df['sentence'] = df['sentence'].apply(lambda x: \"[CLS] \" + x + \" [SEP]\" )\n",
    "    # tokenisation\n",
    "    df['sentence'] = df['sentence'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    # convert token to vocabulary indices\n",
    "    df['id_tokens'] = df['sentence'].apply(lambda x: tokenizer.convert_tokens_to_ids(x))\n",
    "    # convert inputs to PyTorch tensors\n",
    "    df['tokens_tensor'] = df['id_tokens'].apply(lambda x: torch.tensor([x]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "devdata_df = prep_BERT(devdata_df)\n",
    "traindata_df = prep_BERT(traindata_df)\n",
    "\n",
    "# something weird happens during tokenisation, example 135 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files .py from SRC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \"\"\"The Classifier\"\"\"\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    def train(self, trainfile):\n",
    "        \"\"\"Trains the classifier model on the training set stored in file trainfile\"\"\"\n",
    "\n",
    "\n",
    "    def predict(self, datafile):\n",
    "        \"\"\"Predicts class labels for the input instances in file 'datafile'\n",
    "        Returns the list of predicted labels\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "import numpy as np\n",
    "\n",
    "from classifier import Classifier\n",
    "\n",
    "\n",
    "def set_reproducible():\n",
    "    # The below is necessary to have reproducible behavior.\n",
    "    import random as rn\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    # The below is necessary for starting Numpy generated random numbers\n",
    "    # in a well-defined initial state.\n",
    "    np.random.seed(17)\n",
    "    # The below is necessary for starting core Python generated random numbers\n",
    "    # in a well-defined state.\n",
    "    rn.seed(12345)\n",
    "\n",
    "\n",
    "\n",
    "def load_label_output(filename):\n",
    "    with open(filename, 'r', encoding='UTF-8') as f:\n",
    "        return [line.strip().split(\"\\t\")[0] for line in f if line.strip()]\n",
    "\n",
    "\n",
    "\n",
    "def eval_list(glabels, slabels):\n",
    "    if (len(glabels) != len(slabels)):\n",
    "        print(\"\\nWARNING: label count in system output (%d) is different from gold label count (%d)\\n\" % (\n",
    "        len(slabels), len(glabels)))\n",
    "    n = min(len(slabels), len(glabels))\n",
    "    incorrect_count = 0\n",
    "    for i in range(n):\n",
    "        if slabels[i] != glabels[i]: incorrect_count += 1\n",
    "    acc = (n - incorrect_count) / n\n",
    "    return acc*100\n",
    "\n",
    "\n",
    "\n",
    "def train_and_eval(classifier, trainfile, devfile, testfile, run_id):\n",
    "    print(f\"\\nRUN: {run_id}\")\n",
    "    print(\"  %s.1. Training the classifier...\" % str(run_id))\n",
    "    classifier.train(trainfile)\n",
    "    print()\n",
    "    print(\"  %s.2. Eval on the dev set...\" % str(run_id), end=\"\")\n",
    "    slabels = classifier.predict(devfile)\n",
    "    glabels = load_label_output(devfile)\n",
    "    devacc = eval_list(glabels, slabels)\n",
    "    print(\" Acc.: %.2f\" % devacc)\n",
    "    testacc = -1\n",
    "    if testfile is not None:\n",
    "        # Evaluation on the test data\n",
    "        print(\"  %s.3. Eval on the test set...\" % str(run_id), end=\"\")\n",
    "        slabels = classifier.predict(testfile)\n",
    "        glabels = load_label_output(testfile)\n",
    "        testacc = eval_list(glabels, slabels)\n",
    "        print(\" Acc.: %.2f\" % testacc)\n",
    "    print()\n",
    "    return (devacc, testacc)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_reproducible()\n",
    "    n_runs = 5\n",
    "    if len(sys.argv) > 1:\n",
    "        n_runs = int(sys.argv[1])\n",
    "    datadir = \"../data/\"\n",
    "    trainfile =  datadir + \"traindata.csv\"\n",
    "    devfile =  datadir + \"devdata.csv\"\n",
    "    testfile = None\n",
    "    # testfile = datadir + \"testdata.csv\"\n",
    "\n",
    "    # Runs\n",
    "    start_time = time.perf_counter()\n",
    "    devaccs = []\n",
    "    testaccs = []\n",
    "    for i in range(1, n_runs+1):\n",
    "        classifier =  Classifier()\n",
    "        devacc, testacc = train_and_eval(classifier, trainfile, devfile, testfile, i)\n",
    "        devaccs.append(np.round(devacc,2))\n",
    "        testaccs.append(np.round(testacc,2))\n",
    "    print('\\nCompleted %d runs.' % n_runs)\n",
    "    total_exec_time = (time.perf_counter() - start_time)\n",
    "    print(\"Dev accs:\", devaccs)\n",
    "    print(\"Test accs:\", testaccs)\n",
    "    print()\n",
    "    print(\"Mean Dev Acc.: %.2f (%.2f)\" % (np.mean(devaccs), np.std(devaccs)))\n",
    "    print(\"Mean Test Acc.: %.2f (%.2f)\" % (np.mean(testaccs), np.std(testaccs)))\n",
    "    print(\"\\nExec time: %.2f s. ( %d per run )\" % (total_exec_time, total_exec_time / n_runs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
